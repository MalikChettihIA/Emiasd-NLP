# Word2Vec et GloVe : Algorithmes d'Embeddings de Mots

## Introduction

Les algorithmes Word2Vec et GloVe représentent une révolution dans le traitement du langage naturel, introduisant les premiers embeddings de mots vraiment efficaces. Contrairement aux approches classiques comme TF-IDF qui traitent les mots de manière indépendante, ces méthodes capturent les relations sémantiques en représentant les mots dans un espace vectoriel continu de dimension réduite.

## Word2Vec : Apprentissage Contrastif par Contexte

### Principe fondamental

Word2Vec repose sur l'hypothèse distributionnelle : "les mots qui apparaissent dans des contextes similaires ont des significations similaires". Cette intuition linguistique est formalisée dans un algorithme d'apprentissage automatique qui prédit les relations contextuelles entre mots.

### Architecture Skip-gram

L'architecture Skip-gram, l'une des deux variantes de Word2Vec, fonctionne selon le principe suivant : étant donné un mot central, prédire les mots qui l'entourent dans une fenêtre de contexte.

#### Génération des exemples d'entraînement

Pour la phrase "Thou shalt not make a machine", avec une fenêtre de taille 2 :

```
Mot central : "not"
Contexte positif : ["Thou", "shalt", "make", "a"]
```

Chaque paire (mot central, mot de contexte) devient un exemple positif d'entraînement.

#### Formulation mathématique

Pour un mot central $w$ à la position $i$, le contexte positif est défini comme :

$$\mathcal{C}_{pos} = [w_{i-R}, \dots, w_{i-1}, w_{i+1}, \dots, w_{i+R}]$$

où $R$ est la taille de la fenêtre contextuelle.

### Échantillonnage négatif (Negative Sampling)

Pour éviter qu'un modèle apprenne simplement à toujours prédire "vrai", Word2Vec introduit des exemples négatifs : des paires de mots qui n'apparaissent pas normalement ensemble.

**Stratégie** : Pour chaque exemple positif, échantillonner aléatoirement $K$ mots du vocabulaire comme exemples négatifs :

$$\mathcal{C}_{neg} = \{c_1, \dots, c_K\}, \quad c_j \sim \text{Uniform}(\mathcal{V})$$

### Architecture du réseau de neurones

Le modèle est un réseau de neurones simple qui :
1. **Entrée** : Prend deux vecteurs d'embeddings (mot central et mot de contexte)
2. **Traitement** : Calcule une mesure de similarité (souvent le produit scalaire)
3. **Sortie** : Prédit si les deux mots sont contextuellement liés (probabilité entre 0 et 1)

**Fonction objective** : Maximiser la probabilité des paires positives et minimiser celle des paires négatives.

### Architecture CBOW (Continuous Bag of Words)

Alternative à Skip-gram, CBOW prédit le mot central à partir de son contexte. Cette approche est généralement plus rapide mais moins efficace sur des corpus moyens.

### Exemple concret d'entraînement

```python
# Phrase : "Le chat mange des croquettes"
# Fenêtre = 2, mot central = "mange"

Exemples positifs :
("mange", "Le") → 1
("mange", "chat") → 1  
("mange", "des") → 1
("mange", "croquettes") → 1

Exemples négatifs (échantillonnés) :
("mange", "voiture") → 0
("mange", "bleu") → 0
("mange", "ordinateur") → 0
```

## GloVe : Global Vectors for Word Representation

### Approche conceptuelle

GloVe combine les avantages des méthodes de factorisation matricielle (comme LSA) avec ceux des méthodes de fenêtre contextuelle (comme Word2Vec). L'intuition clé est que les statistiques de co-occurrence globales dans un corpus contiennent des informations sémantiques riches.

### Matrice de co-occurrence

GloVe commence par construire une matrice de co-occurrence $X$ où :
- $X_{ij}$ = nombre de fois que le mot $j$ apparaît dans le contexte du mot $i$
- Le contexte est défini par une fenêtre glissante, comme dans Word2Vec

### Fonction objective

GloVe optimise directement les embeddings pour reproduire les logarithmes des probabilités de co-occurrence :

$$J = \sum_{i,j=1}^{V} f(X_{ij})(\mathbf{w}_i^T \mathbf{\tilde{w}}_j + b_i + \tilde{b}_j - \log X_{ij})^2$$

Où :
- $\mathbf{w}_i$ et $\mathbf{\tilde{w}}_j$ sont les vecteurs d'embedding
- $b_i$ et $\tilde{b}_j$ sont des termes de biais
- $f(X_{ij})$ est une fonction de pondération qui réduit l'influence des co-occurrences très fréquentes

### Fonction de pondération

$$f(x) = \begin{cases} 
(\frac{x}{x_{max}})^{\alpha} & \text{si } x < x_{max} \\
1 & \text{sinon}
\end{cases}$$

Typiquement, $x_{max} = 100$ et $\alpha = 0.75$.

### Exemple d'application

```python
# Matrice de co-occurrence simplifiée
#       chat  mange  croquettes
# chat    0     5        2
# mange   5     0        8  
# croquettes 2  8        0

# GloVe apprend des embeddings tels que :
# w_chat · w_mange ≈ log(5)
# w_mange · w_croquettes ≈ log(8)
```

## Propriétés émergentes remarquables

### Relations analogiques

Les embeddings Word2Vec et GloVe capturent des relations analogiques fascinantes :

$$\text{roi} - \text{homme} + \text{femme} \approx \text{reine}$$

Cette propriété émergente suggère que les opérations vectorielles correspondent à des opérations sémantiques.

### Clustering sémantique

Les mots sémantiquement proches se regroupent naturellement dans l'espace vectoriel :

```python
# Voisins de "roi" dans l'espace vectoriel :
# ['prince', 'queen', 'emperor', 'son', 'kingdom', 'throne']
```

## Comparaison Word2Vec vs GloVe

### Word2Vec
**Avantages** :
- Apprentissage efficace sur de gros corpus
- Excellent pour capturer la similarité sémantique
- Méthode d'entraînement simple et intuitive

**Inconvénients** :
- N'utilise que l'information contextuelle locale
- Sensible à la fréquence des mots rares

### GloVe
**Avantages** :
- Utilise les statistiques globales du corpus
- Souvent supérieur sur les tâches d'analogie
- Entraînement plus stable et prévisible

**Inconvénients** :
- Nécessite le calcul préalable de la matrice de co-occurrence
- Plus coûteux en mémoire pour de très gros vocabulaires

## Limites fondamentales des approches statiques

### 1. Absence de contextualisation

**Problème majeur** : Un mot ne peut avoir qu'un seul vecteur d'embedding, indépendamment du contexte.

**Exemple critique** :
```
"La banque de la rivière est couverte de sable"
"Je vais à la banque retirer de l'argent"
```

Le mot "banque" a le même embedding dans les deux phrases, alors que ses significations sont radicalement différentes.

### 2. Gestion défaillante de la polysémie

Les mots polysémiques (ayant plusieurs sens) sont représentés par un vecteur unique qui mélange tous leurs sens, créant des representations ambiguës et parfois contradictoires.

### 3. Ignorance de l'ordre des mots

Les deux méthodes utilisent une approche "sac de mots" au niveau contextuel :

```
"Le chat mange la souris" 
≈ 
"La souris mange le chat"
```

Ces phrases ont des contextes similaires pour Word2Vec/GloVe mais des sens opposés.

### 4. Vocabulaire fixe

**Problème OOV** (Out-of-Vocabulary) : Les mots non vus pendant l'entraînement ne peuvent pas être représentés, limitant l'adaptabilité à de nouveaux domaines ou évolutions linguistiques.

### 5. Insensibilité à la syntaxe

Les embeddings capturent mal les relations syntaxiques complexes :

```
"Marie donne un livre à Jean"
vs
"Jean reçoit un livre de Marie"
```

Bien que sémantiquement liées, ces structures syntaxiques différentes ne sont pas correctement distinguées.

### 6. Biais culturels et sociétaux

Les embeddings héritent des biais présents dans les corpus d'entraînement :

$$\text{homme} - \text{programmeur} + \text{femme} \approx \text{infirmière}$$

Cette analogie problématique reflète les stéréotypes sociétaux présents dans les données.

### 7. Limitation des relations complexes

**Relations non-linéaires** : Les opérations vectorielles linéaires ne peuvent capturer que certains types de relations sémantiques, excluant des relations plus complexes.

**Relations hiérarchiques** : Les taxonomies et ontologies ne sont que partiellement préservées dans l'espace vectoriel.

## Impact et postérité

### Contributions majeures

Word2Vec et GloVe ont posé les fondations conceptuelles des embeddings modernes :

1. **Principe distributionnel formalisé** mathématiquement
2. **Efficacité computationnelle** permettant l'entraînement sur de gros corpus
3. **Transferabilité** des embeddings pré-entraînés vers différentes tâches
4. **Proof of concept** que les relations sémantiques peuvent être capturées vectoriellement

### Évolutions vers les modèles contextuels

Les limites de ces approches ont motivé le développement des modèles contextuels :

**ELMo** : Premiers embeddings bidirectionnels sensibles au contexte
**BERT** : Attention bidirectionnelle et représentations contextuelles profondes
**GPT** : Modèles génératifs avec embeddings contextuels
**Transformers** : Architecture révolutionnaire dépassant les limites séquentielles

## Applications pratiques persistantes

Malgré leurs limites, Word2Vec et GloVe restent utiles pour :

### Systèmes de recommandation
```python
# Recommandation de musique par embedding de chansons
# Chaque playlist = "phrase", chaque chanson = "mot"
similar_songs = model.most_similar(['song_id_123'])
```

### Initialisation d'embeddings
Les embeddings pré-entraînés Word2Vec/GloVe servent souvent d'initialisation pour des modèles plus complexes.

### Applications à ressources limitées
Pour des systèmes avec contraintes computationnelles, ces méthodes offrent un bon compromis performance/efficacité.

### Analyse exploratoire
Excellents outils pour une première exploration des relations sémantiques dans un corpus.

## Considérations pratiques d'implémentation

### Hyperparamètres clés

**Word2Vec** :
- Taille de fenêtre : 5-10 pour la similarité sémantique, 2-3 pour la similarité syntaxique
- Dimension d'embedding : 100-300 (compromis expressivité/efficacité)
- Nombre d'échantillons négatifs : 5-20
- Seuil de sous-échantillonnage : 1e-5 pour les mots fréquents

**GloVe** :
- Paramètres de la fonction de pondération : $x_{max} = 100$, $\alpha = 0.75$
- Nombre d'itérations : 15-50
- Taux d'apprentissage : Décroissant, commençant à 0.05

### Prétraitement optimal

1. **Normalisation** : Conversion en minuscules (attention aux entités nommées)
2. **Tokenisation** : Gestion appropriée de la ponctuation et des contractions
3. **Filtrage par fréquence** : Exclusion des mots très rares (< 5 occurrences) et très fréquents (> 50% des documents)
4. **Gestion des entités** : Traitement spécial pour les noms propres, dates, nombres

## Conclusion

Word2Vec et GloVe marquent une étape cruciale dans l'histoire du NLP, introduisant les concepts fondamentaux d'embeddings vectoriels et d'apprentissage de représentations distribuées. Leurs contributions théoriques et pratiques ont ouvert la voie aux architectures modernes comme les Transformers.

Bien que leurs limites (absence de contextualisation, polysémie non résolue, biais sociétaux) aient motivé le développement de modèles plus sophistiqués, ils conservent leur pertinence dans certains contextes applicatifs et restent des outils pédagogiques exceptionnels pour comprendre les mécanismes sous-jacents des embeddings.

Leur simplicité conceptuelle, combinée à leur efficacité computationnelle, en fait des solutions durables pour des applications spécialisées où les ressources sont limitées ou où une interprétabilité maximale est requise.