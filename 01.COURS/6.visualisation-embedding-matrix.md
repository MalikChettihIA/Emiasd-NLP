# Visualisation des Embeddings : Analyse et Intérêt

## Explication du code de visualisation

### Code analysé
```python
# Plot the embedding matrix of a review
text = "a very bad movie i hated it."
tokens = tokenizer.encode(text)
tokens = torch.tensor([t for t in tokens if t != -1])  # Ignore invalid tokens

with torch.no_grad():
    embedded = classifier.embedding(tokens)  # Shape: (seq_len, d_embed)

plt.figure(figsize=(6, 6))
plt.imshow(embedded.T, cmap="viridis", aspect="auto")
plt.xticks(
    range(len(tokens)), [tokenizer.id_to_token[t.item()] for t in tokens], rotation=45
)
plt.ylabel("Embedding dimension")
plt.colorbar(label="Embedding value")
```

## Décomposition étape par étape

### 1. Préparation des données
```python
text = "a very bad movie i hated it."
tokens = tokenizer.encode(text)
tokens = torch.tensor([t for t in tokens if t != -1])
```

**Fonctionnement** :
- **Tokenisation** : Le texte est converti en identifiants numériques
- **Filtrage** : Les tokens invalides (-1) sont supprimés
- **Tensorisation** : Conversion en tensor PyTorch pour compatibilité GPU

### 2. Extraction des embeddings
```python
with torch.no_grad():
    embedded = classifier.embedding(tokens)  # Shape: (seq_len, d_embed)
```

**Processus** :
- **Mode évaluation** : `torch.no_grad()` désactive le calcul des gradients
- **Lookup d'embeddings** : Chaque token ID récupère son vecteur d'embedding
- **Forme du résultat** : `(nombre_de_mots, dimensions_embedding)`

### 3. Visualisation sous forme de heatmap
```python
plt.imshow(embedded.T, cmap="viridis", aspect="auto")
```

**Choix techniques** :
- **Transposition** : `embedded.T` pour avoir mots en colonnes, dimensions en lignes
- **Colormap "viridis"** : Palette de couleurs perceptuellement uniforme
- **Aspect "auto"** : Ajustement automatique du ratio d'aspect

## Interprétation de la visualisation

### Structure de la matrice visualisée

La heatmap montre une matrice où :
- **Axe horizontal (colonnes)** : Chaque mot de la phrase "a very bad movie i hated it"
- **Axe vertical (lignes)** : Chaque dimension de l'embedding (probablement 50-300 dimensions)
- **Couleurs** : Valeurs numériques des embeddings (-4 à +4 selon l'échelle de couleur)

### Patterns observables

**Colonnes similaires** : Des mots sémantiquement proches devraient avoir des patterns de couleur similaires dans leurs colonnes respectives.

**Répétitions** : Si un mot apparaît plusieurs fois, ses colonnes devraient être identiques (embeddings statiques).

**Variations d'intensité** : Les couleurs représentent l'activation de différentes "propriétés sémantiques" latentes.

## Intérêts pédagogiques et pratiques

### 1. **Compréhension intuitive des embeddings**

**Visualisation concrète** : Au lieu de manipuler des concepts abstraits, on voit littéralement les "nombres" qui représentent les mots.

**Exemple concret** :
- Le mot "bad" pourrait activer fortement certaines dimensions (jaune/vert dans viridis)
- Le mot "movie" pourrait avoir un pattern différent
- Ces différences sont ce qui permet au modèle de distinguer les concepts

### 2. **Analyse de la qualité des embeddings**

**Similarités attendues** : Des mots comme "bad" et "hated" devraient avoir des patterns relativement similaires car ils expriment tous deux une connotation négative.

**Différenciation** : Des mots de catégories différentes ("movie" vs "hated") devraient avoir des patterns distincts.

**Diagnostic de problèmes** :
- Embeddings trop similaires → Modèle sous-entraîné
- Patterns complètement aléatoires → Problème d'initialisation
- Valeurs extrêmes → Possible explosion de gradients

### 3. **Validation du modèle d'embedding**

```python
# Exemple d'analyse comparative
def compare_embeddings(word1, word2, model):
    emb1 = model.embedding(torch.tensor([tokenizer.encode(word1)[0]]))
    emb2 = model.embedding(torch.tensor([tokenizer.encode(word2)[0]]))
    similarity = torch.cosine_similarity(emb1, emb2, dim=1)
    return similarity.item()

# Vérifications attendues :
# compare_embeddings("bad", "terrible") > compare_embeddings("bad", "good")
# compare_embeddings("movie", "film") > compare_embeddings("movie", "banana")
```

### 4. **Débogage et interprétation des modèles**

**Diagnostic de performance** :
- Si le modèle classifie mal, examiner si les embeddings des mots-clés sont cohérents
- Identifier les dimensions qui semblent importantes pour la tâche
- Détecter d'éventuels biais dans les représentations

**Exemple pratique** :
```python
# Analyser l'évolution pendant l'entraînement
def plot_embedding_evolution(model, text, epoch):
    tokens = tokenizer.encode(text)
    embedded = model.embedding(torch.tensor(tokens))
    plt.subplot(2, 3, epoch+1)
    plt.imshow(embedded.T, cmap="viridis")
    plt.title(f"Epoch {epoch}")
```

## Applications avancées de cette visualisation

### 1. **Analyse de domaine spécialisé**

Dans un contexte médical, analyser si le modèle distingue bien :
```python
medical_terms = ["diagnosis", "treatment", "symptom", "patient"]
general_terms = ["happy", "car", "blue", "running"]
# Les embeddings médicaux devraient être plus similaires entre eux
```

### 2. **Étude des biais linguistiques**

```python
# Détecter des biais de genre
male_terms = ["he", "man", "father", "king"]
female_terms = ["she", "woman", "mother", "queen"]
# Analyser si les patterns révèlent des associations problématiques
```

### 3. **Optimisation d'hyperparamètres**

**Dimension d'embedding** : Comparer la richesse des patterns avec différentes dimensions (50, 100, 300).

**Initialisation** : Tester différentes stratégies d'initialisation et leur impact sur les patterns.

## Limitations de cette visualisation

### 1. **Réduction dimensionnelle implicite**

Les couleurs ne peuvent représenter qu'une seule valeur par position, alors que chaque cellule contient une valeur numérique complexe dans un espace haute dimension.

### 2. **Interprétation des dimensions**

Les dimensions d'embedding n'ont pas de signification sémantique directe. Une dimension n'équivaut pas nécessairement à un concept humainement identifiable.

### 3. **Embeddings statiques vs contextuels**

Cette visualisation montre des embeddings statiques (même mot = même représentation). Les modèles modernes (BERT, GPT) génèrent des embeddings contextuels différents selon le contexte.

## Améliorations suggérées

### 1. **Visualisation comparative**

```python
def compare_sentence_embeddings(sentences, model):
    fig, axes = plt.subplots(1, len(sentences), figsize=(15, 5))
    for i, sentence in enumerate(sentences):
        tokens = tokenizer.encode(sentence)
        embedded = model.embedding(torch.tensor(tokens))
        axes[i].imshow(embedded.T, cmap="viridis")
        axes[i].set_title(sentence[:20] + "...")
    plt.show()

# Comparer sentences positives vs négatives
positive_sentences = ["I love this movie", "Great film", "Excellent acting"]
negative_sentences = ["I hate this movie", "Terrible film", "Awful acting"]
```

### 2. **Analyse des similarités**

```python
def plot_similarity_matrix(words, model):
    embeddings = []
    for word in words:
        token = torch.tensor([tokenizer.encode(word)[0]])
        emb = model.embedding(token)
        embeddings.append(emb.squeeze().detach().numpy())
    
    # Matrice de similarité cosinus
    similarities = np.corrcoef(embeddings)
    
    plt.imshow(similarities, cmap="coolwarm")
    plt.xticks(range(len(words)), words, rotation=45)
    plt.yticks(range(len(words)), words)
    plt.colorbar()
    plt.title("Matrice de similarité des embeddings")
```

### 3. **Réduction dimensionnelle pour visualisation**

```python
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

def plot_embedding_space(words, model, method="PCA"):
    embeddings = []
    for word in words:
        token = torch.tensor([tokenizer.encode(word)[0]])
        emb = model.embedding(token).squeeze().detach().numpy()
        embeddings.append(emb)
    
    if method == "PCA":
        reducer = PCA(n_components=2)
    else:
        reducer = TSNE(n_components=2, random_state=42)
    
    reduced = reducer.fit_transform(embeddings)
    
    plt.figure(figsize=(10, 8))
    plt.scatter(reduced[:, 0], reduced[:, 1])
    for i, word in enumerate(words):
        plt.annotate(word, (reduced[i, 0], reduced[i, 1]))
    plt.title(f"Espace d'embeddings réduit ({method})")
    plt.show()
```

## Conclusion

Cette visualisation d'embeddings constitue un outil pédagogique et diagnostique précieux qui permet de :

**Comprendre concrètement** comment les mots sont représentés numériquement dans les modèles de NLP.

**Diagnostiquer** la qualité et cohérence des embeddings appris.

**Valider** que le modèle développe des représentations sensées pour la tâche de classification.

**Déboguer** les problèmes de performance en analysant les représentations internes.

Bien que simplifiée, cette approche offre une fenêtre précieuse sur le "cerveau" du modèle et constitue une étape essentielle pour comprendre et optimiser les architectures d'embedding modernes.