# Word2Vec : Formulation Mathématique Détaillée

## Vue d'ensemble de l'algorithme

Word2Vec est un algorithme d'apprentissage contrastif qui génère des embeddings de mots en exploitant l'hypothèse distributionnelle : **les mots qui apparaissent dans des contextes similaires ont des significations similaires**.

## Étapes détaillées avec formulation mathématique

### 1. Sélection d'un mot et de son contexte

**Processus** : Pour chaque position dans le corpus, on sélectionne un mot central et définit son contexte.

#### Contexte positif
Pour un mot $w$ à la position $i$, le contexte positif est défini comme l'ensemble des mots dans une fenêtre de taille $R$ :

$$\mathcal{C}_{\text{pos}} = [w_{i-R}, \ldots, w_{i-1}, w_{i+1}, \ldots, w_{i+R}]$$

**Exemple concret** :
```
Phrase : "The cat sits on the mat"
Mot central : "sits" (position i=2)
Fenêtre R=2 : 
C_pos = ["The", "cat", "on", "the"]
```

**Propriétés importantes** :
- Le mot central $w_i$ est **exclu** de son propre contexte
- La taille du contexte est $2R$ mots (R à gauche, R à droite)
- En cas de débordement aux bords de la phrase, on prend ce qui est disponible

### 2. Génération d'exemples négatifs

**Problème à résoudre** : Si on n'avait que des exemples positifs (label=1), le modèle apprendrait simplement à toujours prédire "1".

#### Échantillonnage négatif
Pour équilibrer l'entraînement, on génère des exemples négatifs par échantillonnage aléatoire :

$$\mathcal{C}_{\text{neg}} = \{c_1, c_2, \ldots, c_{2KR}\}, \quad c_j \in \mathcal{V}$$

Où :
- $K$ est un multiplicateur (typiquement $K=2$ à $5$)
- $2KR$ est le nombre total d'exemples négatifs
- $\mathcal{V}$ est le vocabulaire complet
- Les $c_j$ sont échantillonnés aléatoirement (souvent avec pondération par fréquence)

**Exemple pratique** :
```
Contexte positif : ["The", "cat", "on", "the"] → 4 exemples positifs
Si K=2, on génère 2×4 = 8 exemples négatifs aléatoirement :
C_neg = ["car", "blue", "running", "elephant", "computer", "happy", "rain", "music"]
```

### 3. Représentation vectorielle des mots

**Tables d'embeddings** : Chaque mot du vocabulaire est associé à un vecteur dans $\mathbb{R}^d$ :

$$\mathbf{v}_w \in \mathbb{R}^d \quad \text{pour le mot central } w$$

#### Matrices de contexte
Les mots de contexte sont également représentés vectoriellement :

$$\mathbf{v}_{\mathcal{C}_{\text{pos}}} \in \mathbb{R}^{2R \times d}$$
$$\mathbf{v}_{\mathcal{C}_{\text{neg}}} \in \mathbb{R}^{2KR \times d}$$

**Note importante** : Dans l'implémentation classique de Word2Vec, on utilise en fait **deux matrices d'embeddings distinctes** :
- Une matrice $\mathbf{W}_{\text{in}}$ pour les mots centraux
- Une matrice $\mathbf{W}_{\text{out}}$ pour les mots de contexte

### 4. Calcul de similarité

**Mesure de proximité** : Pour chaque paire (mot central, mot de contexte), on calcule leur similarité via le produit scalaire :

$$s = \mathbf{v}_c \cdot \mathbf{v}_w$$

Où :
- $\mathbf{v}_w$ est l'embedding du mot central
- $\mathbf{v}_c$ est l'embedding du mot de contexte
- $s$ est un score de similarité (non borné)

**Interprétation géométrique** :
- Score élevé → vecteurs alignés → mots sémantiquement liés
- Score faible → vecteurs orthogonaux → mots non liés
- Score négatif → vecteurs opposés → mots antagonistes

### 5. Fonction objective et entraînement

#### Probabilité de co-occurrence
Le modèle prédit la probabilité qu'un mot de contexte $c$ apparaisse avec le mot central $w$ :

$$P(c|w) = \frac{\exp(\mathbf{v}_c \cdot \mathbf{v}_w)}{\sum_{c' \in \mathcal{V}} \exp(\mathbf{v}_{c'} \cdot \mathbf{v}_w)}$$

**Problème computationnel** : Le dénominateur nécessite de sommer sur tout le vocabulaire ($|\mathcal{V}|$ peut être > 100k mots) → **très coûteux**.

#### Approximation par échantillonnage négatif
Au lieu de la softmax complète, Word2Vec utilise une approximation binaire :

$$P(\text{contexte}=1|w,c) = \sigma(\mathbf{v}_c \cdot \mathbf{v}_w)$$

Où $\sigma(x) = \frac{1}{1+e^{-x}}$ est la fonction sigmoïde.

#### Fonction de perte (Skip-gram with Negative Sampling)

La fonction objective complète à **maximiser** est :

$$\mathcal{L} = \sum_{(w,c) \in \mathcal{D}_{\text{pos}}} \log \sigma(\mathbf{v}_c \cdot \mathbf{v}_w) + \sum_{(w,c') \in \mathcal{D}_{\text{neg}}} \log \sigma(-\mathbf{v}_{c'} \cdot \mathbf{v}_w)$$

**Décomposition** :
- **Premier terme** : Maximise la probabilité des paires positives (vraies co-occurrences)
- **Second terme** : Minimise la probabilité des paires négatives (co-occurrences aléatoirement échantillonnées)

### 6. Algorithme d'optimisation

#### Gradient Stochastique
Pour chaque exemple d'entraînement $(w, c, y)$ où $y \in \{0,1\}$ :

**Gradients** :
$$\frac{\partial \mathcal{L}}{\partial \mathbf{v}_w} = (y - \sigma(\mathbf{v}_c \cdot \mathbf{v}_w)) \mathbf{v}_c$$
$$\frac{\partial \mathcal{L}}{\partial \mathbf{v}_c} = (y - \sigma(\mathbf{v}_c \cdot \mathbf{v}_w)) \mathbf{v}_w$$

**Mises à jour** :
$$\mathbf{v}_w \leftarrow \mathbf{v}_w + \alpha \frac{\partial \mathcal{L}}{\partial \mathbf{v}_w}$$
$$\mathbf{v}_c \leftarrow \mathbf{v}_c + \alpha \frac{\partial \mathcal{L}}{\partial \mathbf{v}_c}$$

Où $\alpha$ est le taux d'apprentissage.

## Exemple numérique complet

### Configuration
- Vocabulaire : $\mathcal{V} = \{\text{"cat"}, \text{"dog"}, \text{"bird"}, \text{"car"}, \text{"blue"}\}$
- Dimension d'embedding : $d = 3$
- Fenêtre : $R = 1$
- Multiplicateur négatif : $K = 1$

### Phrase d'entraînement
```
"cat sits bird"
```

### Génération des exemples
**Mot central** : "sits" (index 1)
**Contexte positif** : $\mathcal{C}_{\text{pos}} = [\text{"cat"}, \text{"bird"}]$
**Contexte négatif** : $\mathcal{C}_{\text{neg}} = [\text{"dog"}, \text{"car"}]$ (échantillonnés aléatoirement)

### Exemples d'entraînement générés
```
("sits", "cat", 1)    # Exemple positif
("sits", "bird", 1)   # Exemple positif  
("sits", "dog", 0)    # Exemple négatif
("sits", "car", 0)    # Exemple négatif
```

### Embeddings initiaux (aléatoires)
```
v_sits = [0.1, 0.2, -0.1]
v_cat  = [0.3, -0.1, 0.2]
v_bird = [0.2, 0.1, 0.3]
v_dog  = [-0.1, 0.3, -0.2]
v_car  = [0.4, -0.2, 0.1]
```

### Calculs pour l'exemple ("sits", "cat", 1)
**Score** : $s = \mathbf{v}_{\text{sits}} \cdot \mathbf{v}_{\text{cat}} = 0.1×0.3 + 0.2×(-0.1) + (-0.1)×0.2 = 0.03 - 0.02 - 0.02 = -0.01$

**Probabilité** : $P = \sigma(-0.01) = \frac{1}{1+e^{0.01}} \approx 0.497$

**Erreur** : $\text{erreur} = 1 - 0.497 = 0.503$

**Mise à jour** (avec $\alpha = 0.1$) :
```
v_sits ← [0.1, 0.2, -0.1] + 0.1 × 0.503 × [0.3, -0.1, 0.2] 
       = [0.115, 0.195, -0.09]
       
v_cat  ← [0.3, -0.1, 0.2] + 0.1 × 0.503 × [0.1, 0.2, -0.1]
       = [0.305, -0.09, 0.195]
```

## Variantes algorithmiques

### Skip-gram vs CBOW

**Skip-gram (présenté ci-dessus)** :
- Prédit le contexte à partir du mot central
- Meilleur pour corpus moyens et grands
- Plus sensible aux mots rares

**CBOW (Continuous Bag of Words)** :
- Prédit le mot central à partir du contexte
- Plus rapide à entraîner
- Meilleur pour petits corpus

### Stratégies d'échantillonnage négatif

**Échantillonnage uniforme** : Chaque mot a la même probabilité d'être sélectionné.

**Échantillonnage pondéré par fréquence** : 
$$P(w) \propto f(w)^{3/4}$$

Où $f(w)$ est la fréquence du mot $w$. L'exposant $3/4$ réduit le biais vers les mots très fréquents.

**Sous-échantillonnage des mots fréquents** : Les mots très communs (articles, prépositions) sont parfois ignorés avec une certaine probabilité.

## Paramètres d'entraînement typiques

### Hyperparamètres recommandés
- **Dimension d'embedding** : 100-300 (compromis entre expressivité et efficacité)
- **Taille de fenêtre** : 5-10 (plus large = plus sémantique, plus étroite = plus syntaxique)
- **Nombre d'échantillons négatifs** : 5-20
- **Taux d'apprentissage** : 0.025 (décroissant pendant l'entraînement)
- **Nombre d'époques** : 5-15

### Complexité computationnelle
- **Par exemple d'entraînement** : $O(d \times (1 + k))$ où $k$ est le nombre d'échantillons négatifs
- **Total** : $O(|D| \times d \times k)$ où $|D|$ est la taille du corpus
- **Avantage majeur** : Linéaire en la taille du vocabulaire (vs quadratique pour les approches matricielles)

## Propriétés émergentes

### Relations analogiques
L'entraînement Word2Vec fait émerger des relations vectorielles remarquables :

$$\mathbf{v}_{\text{roi}} - \mathbf{v}_{\text{homme}} + \mathbf{v}_{\text{femme}} \approx \mathbf{v}_{\text{reine}}$$

**Explication** : Les différences vectorielles capturent des "directions sémantiques" (genre, temporalité, etc.).

### Clustering sémantique
Les mots sémantiquement proches se regroupent naturellement dans l'espace vectoriel :
- Distance cosinus faible → concepts similaires
- Clusters émergents par domaines (couleurs, animaux, verbes d'action, etc.)

## Applications étendues

### Systèmes de recommandation
L'algorithme s'adapte naturellement à d'autres domaines :
```python
# Recommandation musicale : playlist = phrase, chanson = mot
playlists = [["song1", "song2", "song3"], ["song2", "song4", "song5"]]
model = Word2Vec(playlists, vector_size=32, window=20)
```

### Embeddings de concepts non-linguistiques
- **Code source** : fonction = mot, fichier = phrase
- **Navigation web** : page = mot, session = phrase  
- **Bioinformatique** : protéine = mot, séquence = phrase

## Conclusion

La formulation mathématique de Word2Vec révèle la beauté de sa simplicité : en apprenant simplement à distinguer les co-occurrences réelles des co-occurrences aléatoires, l'algorithme développe une compréhension sophistiquée des relations sémantiques. Cette approche a ouvert la voie aux embeddings modernes et reste un pilier conceptuel du traitement automatique du langage.