# Analyse du ProbaClassifier - Classification Probabiliste

## Vue d'ensemble

Le `ProbaClassifier` est une implémentation personnalisée d'un classificateur de texte basé sur les fréquences de mots. Cette classe hérite de `BaseTextClassifier` et implémente une approche probabiliste qui s'inspire des principes du classificateur Naive Bayes, mais avec une variante dans le calcul des scores.

## Architecture de la classe

### Initialisation
```python
def __init__(self, n_classes):
    self.n_classes = n_classes
    self.tokenizer = WhiteSpaceTokenizer()
```

**Paramètres** :
- `n_classes` : Nombre de classes de classification (ex: 2 pour sentiment positif/négatif)
- `tokenizer` : Utilisé pour convertir le texte en tokens numériques

## Analyse détaillée des méthodes

### 1. Méthode `get_word_count(X, y)`

Cette méthode calcule les statistiques fondamentales nécessaires à la classification :

```python
def get_word_count(self, X, y):
    words_count = np.zeros((self.n_classes, self.tokenizer.num_words), dtype=int)
    for i in range(len(X)):
        tokens = self.tokenizer.encode(X[i])
        for w in tokens:
            if w != -1:
                words_count[y[i], w] += 1
    frequencies = words_count / words_count.sum(axis=1, keepdims=True)
    return words_count, frequencies
```

**Fonctionnement** :

1. **Création de la matrice de comptage** : `words_count` est une matrice de dimensions `(n_classes, vocabulaire_size)` où chaque élément `[classe, mot]` contient le nombre d'occurrences du mot dans cette classe.

2. **Comptage par classe** : Pour chaque document d'entraînement, le code :
   - Tokenise le texte en identifiants numériques
   - Incrémente le compteur pour chaque mot dans la classe correspondante
   - Ignore les tokens invalides (`w != -1`)

3. **Calcul des fréquences** : Les comptages bruts sont normalisés en divisant par la somme totale de mots par classe, créant des distributions de probabilité.

**Exemple concret** :
```
Documents d'entraînement:
- "film excellent" → classe 1 (positif)
- "film nul" → classe 0 (négatif)
- "excellent acteur" → classe 1 (positif)

Matrice words_count:
           film  excellent  nul  acteur
Classe 0    1       0       1      0
Classe 1    1       2       0      1

Matrice frequencies (normalisées):
           film  excellent  nul  acteur
Classe 0   0.5     0.0     0.5    0.0
Classe 1   0.25   0.5      0.0   0.25
```

### 2. Méthode `fit(X, y)`

```python
def fit(self, X, y):
    self.tokenizer.fit(X)
    word_count, frequencies = self.get_word_count(X, y)
    self.frequencies = frequencies
    self.words_count = word_count
```

**Fonctionnalités** :
- **Entraînement du tokenizer** : Construction du vocabulaire à partir des données
- **Calcul des statistiques** : Stockage des comptages et fréquences pour la prédiction
- **Mémorisation** : Sauvegarde des paramètres nécessaires à la classification

### 3. Méthode `predict(text)` - Cœur algorithmique

```python
def predict(self, text: str) -> int:
    global_count = self.words_count.sum(axis=0)
    global_freq = global_count / global_count.sum()
    words = self.tokenizer.encode(text)
    words = np.array([w for w in words if w != -1])
    words_count_per_class = self.frequencies[:, words] / global_freq[words]
    scores = words_count_per_class.sum(axis=1)
    return np.argmax(scores)
```

**Analyse step-by-step** :

#### Étape 1 : Calcul des fréquences globales
```python
global_count = self.words_count.sum(axis=0)
global_freq = global_count / global_count.sum()
```

**Objectif** : Calculer la fréquence de chaque mot dans l'ensemble du corpus, toutes classes confondues. Cela sert de terme de normalisation.

#### Étape 2 : Tokenisation du texte à prédire
```python
words = self.tokenizer.encode(text)
words = np.array([w for w in words if w != -1])
```

**Fonctionnement** : Conversion du texte en identifiants numériques, filtrage des tokens invalides.

#### Étape 3 : Calcul du score pondéré (Innovation clé)
```python
words_count_per_class = self.frequencies[:, words] / global_freq[words]
scores = words_count_per_class.sum(axis=1)
```

**Formule mathématique** :
Pour chaque classe $c$ et chaque mot $w$ dans le texte à prédire :

$$\text{score}(c) = \sum_{w \in \text{document}} \frac{P(w|c)}{P(w)}$$

Où :
- $P(w|c)$ = fréquence du mot $w$ dans la classe $c$ (de `self.frequencies`)
- $P(w)$ = fréquence globale du mot $w$ dans tout le corpus

#### Étape 4 : Sélection de la classe
```python
return np.argmax(scores)
```

La classe avec le score le plus élevé est choisie.

## Intuition algorithmique

### Principe de pondération TF-IDF inversé

Le classificateur utilise un principe inspiré de TF-IDF mais appliqué différemment :

**Pondération** : $\frac{P(w|c)}{P(w)}$

- **Numérateur** `P(w|c)` : Plus un mot est fréquent dans une classe, plus il contribue au score de cette classe
- **Dénominateur** `P(w)` : Plus un mot est commun dans tout le corpus, moins il est discriminant

**Exemple illustratif** :
```
Mot "excellent" :
- P("excellent"|positif) = 0.5 (très fréquent dans les avis positifs)
- P("excellent") = 0.1 (rare globalement)
- Contribution = 0.5/0.1 = 5.0 (très discriminant pour positif)

Mot "film" :
- P("film"|positif) = 0.25 
- P("film") = 0.4 (très commun)
- Contribution = 0.25/0.4 = 0.625 (peu discriminant)
```

## Comparaison avec Naive Bayes classique

### Naive Bayes standard
$$P(c|\text{document}) \propto P(c) \prod_{w} P(w|c)$$

### ProbaClassifier
$$\text{score}(c) = \sum_{w} \frac{P(w|c)}{P(w)}$$

**Différences clés** :

1. **Multiplication vs Addition** : Naive Bayes multiplie les probabilités, ProbaClassifier les additionne
2. **Normalisation** : Division par la fréquence globale au lieu d'utiliser la probabilité a priori des classes
3. **Log-vraisemblance** : Pas de passage au logarithme dans ProbaClassifier

## Avantages et inconvénients

### Avantages

**Simplicité conceptuelle** : L'algorithme est facile à comprendre et interpréter.

**Robustesse aux mots rares** : La normalisation par la fréquence globale évite que des mots très rares dominent complètement.

**Efficacité computationnelle** : Calculs vectoriels rapides, pas de produits de nombreuses petites probabilités.

**Pas d'hypothèse d'indépendance forte** : Contrairement à Naive Bayes, n'assume pas explicitement l'indépendance conditionnelle des mots.

### Inconvénients

**Justification théorique limitée** : Contrairement à Naive Bayes, pas de fondement probabiliste rigoureux.

**Sensibilité aux mots très fréquents** : Les mots common peuvent avoir des contributions négatives si plus fréquents globalement que dans une classe.

**Pas de gestion explicite des mots inconnus** : Les mots non vus en entraînement sont ignorés.

**Absence de lissage** : Risque de division par zéro si un mot n'apparaît jamais globalement.

## Cas d'usage appropriés

### Adapté pour :
- **Classification de sentiment** avec vocabulaire bien défini
- **Categorisation de documents** où les mots-clés sont discriminants
- **Prototypage rapide** de systèmes de classification
- **Données équilibrées** entre classes

### Moins adapté pour :
- **Corpus avec vocabulaire très variable** 
- **Classification fine avec nuances subtiles**
- **Données très déséquilibrées entre classes**
- **Textes très courts** avec peu de mots discriminants

## Amélioration possibles

### 1. Ajout de lissage de Laplace
```python
frequencies = (words_count + alpha) / (words_count.sum(axis=1, keepdims=True) + alpha * vocab_size)
```

### 2. Gestion des mots inconnus
```python
# Ajouter un token spécial <UNK> pour les mots non vus
if w not in self.vocabulary:
    w = self.unk_token_id
```

### 3. Normalisation par longueur de document
```python
scores = scores / len(words)  # Éviter le biais vers les documents longs
```

### 4. Incorporation des probabilités a priori des classes
```python
class_priors = np.bincount(y_train) / len(y_train)
scores = scores * class_priors
```

## Conclusion

Le `ProbaClassifier` représente une approche pragmatique de classification de texte qui combine simplicité d'implémentation et efficacité pratique. Bien qu'il s'écarte des fondements théoriques stricts des méthodes bayésiennes, son mécanisme de pondération TF-IDF inversé peut être très efficace pour des tâches de classification où les mots-clés discriminants sont clairement identifiables.

Cette implémentation illustre parfaitement comment des heuristiques simples peuvent parfois rivaliser avec des méthodes plus sophistiquées, surtout dans des contextes où la rapidité de développement et l'interprétabilité sont prioritaires.