# TF-IDF : Term Frequency - Inverse Document Frequency

## Introduction

TF-IDF (Term Frequency - Inverse Document Frequency) est un algorithme fondamental en traitement automatique du langage naturel (NLP) et en recherche d'information. Il permet de transformer un corpus de documents textuels en représentations numériques vectorielles, en attribuant un poids à chaque terme selon son importance dans un document particulier par rapport à l'ensemble du corpus.

## Intuition de base

L'algorithme TF-IDF repose sur deux intuitions complémentaires qui forment le cœur de sa logique :

**Première intuition** : Si un mot apparaît fréquemment dans un document spécifique, il est probablement important pour caractériser ce document. Par exemple, dans un document traitant de cuisine, les termes "recette", "ingrédients" ou "cuisson" apparaîtront naturellement plus souvent.

**Deuxième intuition** : Si un mot apparaît dans de nombreux documents du corpus, il perd de sa valeur discriminante. Les mots comme "le", "de", "et" ou "dans" apparaissent dans pratiquement tous les textes français et n'apportent donc pas d'information spécifique sur le contenu d'un document particulier.

TF-IDF combine ces deux concepts pour créer un système de pondération qui valorise les mots à la fois fréquents dans un document donné ET rares dans l'ensemble du corpus.

## Formulation mathématique

### 1. Term Frequency (TF)

La fréquence des termes mesure l'importance d'un mot dans un document particulier :

$$\text{TF}(t, d) = \frac{f_{t,d}}{\sum_{t' \in d} f_{t',d}}$$

Où :
- $f_{t,d}$ est le nombre d'occurrences du terme $t$ dans le document $d$
- $\sum_{t' \in d} f_{t',d}$ est le nombre total de mots dans le document $d$

Cette normalisation évite qu'un document plus long ait automatiquement des scores TF plus élevés.

### 2. Inverse Document Frequency (IDF)

L'IDF mesure la rareté d'un terme dans l'ensemble du corpus :

$$\text{IDF}(t) = \log\left(\frac{N + 1}{\text{DF}(t) + 1}\right) + 1$$

Où :
- $N$ est le nombre total de documents dans le corpus
- $\text{DF}(t)$ est le nombre de documents contenant le terme $t$
- Les "+1" au numérateur et dénominateur évitent la division par zéro

### 3. Score TF-IDF final

$$\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)$$

## Exemple concret

Supposons un corpus de 3 documents :
- Document 1 : "Le chat mange des croquettes"
- Document 2 : "Le chien aime les croquettes"  
- Document 3 : "Les animaux domestiques mangent"

Pour le terme "croquettes" dans le Document 1 :

**Calcul TF** :
- "croquettes" apparaît 1 fois dans le Document 1
- Le Document 1 contient 5 mots au total
- $\text{TF}(\text{croquettes}, \text{doc1}) = \frac{1}{5} = 0.2$

**Calcul IDF** :
- Le corpus contient 3 documents ($N = 3$)
- "croquettes" apparaît dans 2 documents ($\text{DF}(\text{croquettes}) = 2$)
- $\text{IDF}(\text{croquettes}) = \log\left(\frac{3+1}{2+1}\right) + 1 = \log(1.33) + 1 \approx 1.29$

**Score TF-IDF final** :
$$\text{TF-IDF}(\text{croquettes}, \text{doc1}) = 0.2 \times 1.29 = 0.258$$

Comparativement, pour le mot "le" qui apparaît dans tous les documents, l'IDF serait beaucoup plus faible, réduisant son importance globale.

## Implémentation pratique

Voici comment implémenter TF-IDF à partir d'une matrice de comptage de mots :

```python
import numpy as np

def word_count_to_tfidf(word_count_matrix):
    # Calcul des fréquences des termes (TF)
    term_frequencies = word_count_matrix / word_count_matrix.sum(axis=1, keepdims=True)
    
    # Calcul des fréquences documentaires (DF)
    document_frequencies = np.count_nonzero(word_count_matrix > 0, axis=0)
    
    # Calcul des IDF
    num_documents = word_count_matrix.shape[0]
    idf = np.log((num_documents + 1) / (document_frequencies + 1)) + 1
    
    # Calcul final TF-IDF
    tfidf_matrix = term_frequencies * idf
    return tfidf_matrix
```

## Applications pratiques

### 1. Recherche d'information et moteurs de recherche

TF-IDF est historiquement l'un des algorithmes fondamentaux des moteurs de recherche. Lorsqu'un utilisateur saisit une requête, le système calcule le score TF-IDF de chaque terme de la requête dans chaque document de la base, permettant de classer les documents par ordre de pertinence.

**Exemple** : Pour une requête "recettes végétariennes", un document culinaire mentionnant fréquemment ces termes aura un score élevé, tandis qu'un document généraliste les mentionnant occasionnellement aura un score plus faible.

### 2. Classification de documents

TF-IDF transforme les documents en vecteurs numériques que les algorithmes de machine learning peuvent traiter. Chaque document devient un vecteur de dimension égale à la taille du vocabulaire, où chaque composante représente le score TF-IDF d'un terme.

### 3. Détection de documents similaires

En calculant la similarité cosinus entre les vecteurs TF-IDF de deux documents, on peut mesurer leur proximité thématique :

```python
from sklearn.metrics.pairwise import cosine_similarity

# Calcul de similarité entre deux documents
similarity = cosine_similarity(tfidf_vector_doc1, tfidf_vector_doc2)
```

### 4. Topic modeling et clustering

Dans des frameworks comme BERTopic, TF-IDF est utilisé sous une forme adaptée (c-TF-IDF) pour représenter des topics. Au lieu de calculer les fréquences au niveau des documents, elles sont calculées au niveau des clusters de documents, permettant d'identifier les mots les plus caractéristiques de chaque topic.

### 5. Extraction de mots-clés

TF-IDF peut identifier automatiquement les termes les plus représentatifs d'un document en sélectionnant ceux ayant les scores les plus élevés.

## Avantages et limites

### Avantages

**Simplicité conceptuelle** : L'algorithme est intuitive et facile à comprendre, ce qui facilite son interprétation et son débogage.

**Efficacité computationnelle** : Les calculs sont relativement rapides même sur de gros corpus, utilisant principalement des opérations matricielles optimisées.

**Robustesse** : TF-IDF fonctionne bien sur une grande variété de types de textes et de domaines sans nécessiter de paramétrage complexe.

**Interprétabilité** : Les scores peuvent être facilement analysés et expliqués, contrairement aux embeddings neuronaux.

### Limites

**Absence de contexte sémantique** : TF-IDF traite chaque mot indépendamment et ne capture pas les relations sémantiques. Les mots "voiture" et "automobile" sont traités comme complètement différents.

**Ordre des mots ignoré** : L'approche "sac de mots" perd l'information sur l'ordre des termes, ce qui peut être crucial pour certaines tâches.

**Gestion des synonymes** : Les synonymes ne sont pas reconnus comme liés, ce qui peut fragmenter l'information.

**Sensibilité à la taille du corpus** : Les scores IDF dépendent fortement de la composition du corpus d'entraînement.

## Évolutions et alternatives modernes

Bien que TF-IDF reste un outil fondamental, il a été enrichi ou remplacé par des approches plus sophistiquées :

**N-grammes TF-IDF** : Extension pour capturer des séquences de mots (bigrammes, trigrammes) plutôt que des mots isolés.

**Word embeddings** : Des modèles comme Word2Vec ou GloVe capturent les relations sémantiques entre mots dans des espaces vectoriels continus.

**Transformers** : Des architectures comme BERT génèrent des représentations contextuelles où le même mot peut avoir différentes représentations selon le contexte.

**Variantes spécialisées** : c-TF-IDF pour le topic modeling, où les calculs se font au niveau des clusters plutôt que des documents individuels.

## Conseils pratiques d'utilisation

Pour optimiser l'usage de TF-IDF dans vos projets :

**Prétraitement approprié** : Appliquez une normalisation du texte (minuscules, suppression de la ponctuation), une lemmatisation et une suppression des mots vides avant le calcul TF-IDF.

**Choix du vocabulaire** : Limitez le vocabulaire en excluant les termes très rares (présents dans moins de 2-3 documents) et très fréquents (présents dans plus de 95% des documents).

**Validation croisée** : Utilisez des techniques de validation croisée pour évaluer les performances, car les scores TF-IDF dépendent du corpus d'entraînement.

**Combinaison avec d'autres techniques** : TF-IDF peut servir de baseline ou être combiné avec des approches plus avancées pour créer des systèmes hybrides.

TF-IDF reste aujourd'hui un pilier du traitement automatique du langage, offrant un excellent équilibre entre simplicité, efficacité et performance pour de nombreuses tâches de base en NLP.