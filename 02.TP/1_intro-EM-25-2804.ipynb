{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "s-8ltYHfbs5B",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gdown\n",
    "\n",
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping already downloaded file IMDB_Dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'IMDB_Dataset.csv'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_path = 'https://drive.google.com/uc?id='\n",
    "file_id = '1tqZpPvvyluyu7VVvk99tKpJd4cIkS4yi'\n",
    "output_name = 'IMDB_Dataset.csv'\n",
    "gdown.download(google_path+file_id,output_name,quiet=False,resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction : Machine learning for NLP\n",
    "\n",
    "üöß **Question** üöß\n",
    "\n",
    "Why would one need NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "üöß **Question** üöß\n",
    "\n",
    "Can you give two main domain of NLP applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### First application: text classification\n",
    "\n",
    "- Goal of text classification: assign a label to a text.\n",
    "- We actually want to model $p(y | x)$ where $x$ is the text and $y$ is the label.\n",
    "- Let's call $\\hat{p}$ our model. $\\hat{p}$ is a function that takes a text as input and outputs a label.\n",
    "\n",
    "üöß **Question** üöß\n",
    "\n",
    "How would you model $\\hat{p}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Raw data to dataset\n",
    "First take a look at the data ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.&lt;br /&gt;&lt;br /&gt;It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.&lt;br /&gt;&lt;br /&gt;I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. &lt;br /&gt;&lt;br /&gt;The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. &lt;br /&gt;&lt;br /&gt;The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.&lt;br /&gt;&lt;br /&gt;This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.&lt;br /&gt;&lt;br /&gt;This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his parents are fighting all the time.&lt;br /&gt;&lt;br /&gt;This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.&lt;br /&gt;&lt;br /&gt;OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing &amp; arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.&lt;br /&gt;&lt;br /&gt;3 out of 10 just for the well playing parents &amp; descent dialogs. As for the shots with Jake: just ignore them.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. &lt;br /&gt;&lt;br /&gt;This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.&lt;br /&gt;&lt;br /&gt;The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.&lt;br /&gt;&lt;br /&gt;The acting is good under Mr. Mattei's direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.&lt;br /&gt;&lt;br /&gt;We wish Mr. Mattei good luck and await anxiously for his next work.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              review  \\\n",
       "0  One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                              Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei's direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.   \n",
       "\n",
       "  sentiment  \n",
       "0  positive  \n",
       "1  positive  \n",
       "2  positive  \n",
       "3  negative  \n",
       "4  positive  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(output_name, nrows=5000)\n",
    "# Print some data. Make sure all text is printed with pandas, with word wrap\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need input and output: $X$ and $y$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"review\"]\n",
    "y = (df[\"sentiment\"] == \"positive\").astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test (and validation)\n",
    "\n",
    "üöß **Question** üöß\n",
    "\n",
    "It is important to do it before the preprocessing. Why now ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1234\n",
    ")\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After many, many years I saw again this beautiful love story, thinking about how would I, half a century after, react to a film which made so many girls cry and sigh at that time, when I was just an male adolescent trying to understand women's behaviors, in a small city in Brazil.<br /><br />This time, however, what caught my attention in the film was something very different, namely the insistence with which the physician Dr. Han Suyin (Jennifer Jones) makes clear to the journalist Mark Elliott (William Holden) her special ethically condition as an Eurasian. In fact, she is constantly putting emphasis on this point in their relationship, repeating she is willing to assume her love for him and carry it on in a \"occidental way\", provided that, by doing so, she is not betraying her Chinese side. Its seems to the spectator that Suyin is eagerly making efforts to establish a very subtle conciliation between those two unstable and opposite aspects of her culture, for they will immediately engage in overt conflict in her mind at a minimum failure in her attempts to control them.<br /><br />Therefore, Suyin's attitudes always leave poor Elliott ¬ñ a determined, brave and extremely practical man ¬ñ anxious and perplexed, without knowing how much importance to give to her words. For him, whose love for her is plain and simple, the situation is totally clear: if we love each other, let us make a couple and begin immediately a life together. \"Not so fast\", is what she seems, verbally and non-verbally, to answer him all the time.<br /><br />In fact, Suyin's Chinese portion would never allow her such a level of pragmatism. And, as she goes on and on reinforcing this much aimed equilibrium between those two worlds inside herself, she also frequently signals to him that also a very peculiar trait of Chinese culture is deeply rooted in her mind, namely the constant \"raids\" on the real world by invisible beings from an spiritual or non-physical world. For Suyin is always alerting Elliott about how dangerous is life, not because of any objective and concrete threat (as would be the perpetuation of the English colonialism or the eminence of a Japanese invasion), but due to the threats of plenty of cruel and harmful gods and other mystical and mythical beings over the poor, fearful and vulnerable human beings.<br /><br />In fact, it looks like a whole bunch of Chinese deities are permanently on the watch to make people's life totally miserable. Because of that, mothers must dress their precious male babies in girls clothes, so that they are not taken away by jealous gods; everyone should always be ready to make loud noises to send the clouds away, in order to avoid their covering the sight of the moon; peasants are advised that they should shout loudly \"The rice is bad! The rice is bad!\" to protect their crops from being stolen by deities; and, in a funeral, it is recommended that the dead's family be isolated from the other people by curtains, so that the gods don't take advantage of their sorrow and fragility.<br /><br />In other words, Suyin introduces us to a culture in which the supernatural has a real existence, as if a rather disturbing pantheon of malign and sadistic gods are always on the verge of negatively interfering with the most banal acts in anyone's daily life.<br /><br />As the story takes place in Hong Kong in 1949, it should be clear that China really was, at that time, almost a semi-feudal society, while the country from which Elliott had come from was not yet dominated by the fierce capitalism that, launched by the USA after the first oil shock in 1973, took charge of the whole world. Therefore, at least in one aspect, both sides of Suyin's Eurasian personality were still much more innocent than they would be today.<br /><br />A lot of History came into being since those old days. As to China, the main fact is that, after several phases of a communist regime, the country finally reached, in the last two decades, the condition of a very aggressive economy much more properly described as State capitalism. And, what happened to that old spirituality that so much enthralled Suyin in Hong Kong, in 1949, and with which she used to impress so much an impassioned Elliott, under that tree on the hill behind the hospital? It is gone, completely gone! In brief, if that story took place today, Elliott would not find it necessary to go to China to propose to Suyin in the presence of the Third Uncle and her entire family. In fact, both men would now be incomparably closer to one another, in their huge pragmatism, talking business as usual!\n",
      "\n",
      "I'm disappointed that Reiser (who wrote the film) felt the need to use so much profanity for no reason whatsoever. Maybe that's his idea of \"adult\" films, plenty of nasty words with bathroom humor thrown in? I thought better of him and think less of him for this movie.<br /><br />Falk's acting and some moments of humor as well as some possibly important themes are what made me give it such a high rating.<br /><br />This might be a good movie for adult children to watch and laugh over about their own folks and their foibles. But the lack of consideration for audience families seriously detriments what could have been a family film but fails. Certainly not worth spending money on, though it might be worth a watch for free on television.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore a couple of input texts\n",
    "n=2\n",
    "selection = np.random.choice(len(X_train),n)\n",
    "for i in  selection:\n",
    "    print(X_train[i]+\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöß **Question** üöß\n",
    "\n",
    "Is it what we want  ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##¬†Pre-processing and Tokenization\n",
    "\n",
    "Tokenizing a text means splitting it into words.\n",
    "\n",
    "This is not as easy as it seems. For instance, how would you tokenize the following text?\n",
    "\n",
    "```\n",
    "I'm a student. Are you a student? Alex's course has already started!\n",
    "```\n",
    "- We can naively split on spaces.\n",
    "- We can use a library like `nltk`, that incorporates more rules to split the text (`spacy`, `beautiful soup`)\n",
    "- We can include some *pre-processing*.\n",
    "- Deal with encoding issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    # string normalization.\n",
    "    text = unicodedata.normalize(\"NFD\", text).encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "    # replace remove html stuffs.\n",
    "    text = re.sub(\"<.*?>\", \" \", text)\n",
    "\n",
    "    # remove non alpha numeric character, e.g punctuations ! \n",
    "    text = re.sub(\"[^a-z0-9]\", \" \", text)\n",
    "\n",
    "    # replace numbers by the <NUM> token.\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "\n",
    "    # remove double whitespaces.\n",
    "    text = re.sub(\" +\", \" \", text.strip())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i lived in san francisco for years'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_text = \"√è   l√Æved  in    San-Fran√ßisco...  ! for <bf>12 years</bf>.\"\n",
    "preprocess_text(noisy_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after many many years i saw again this beautiful love story thinking about how would i half a century after react to a film which made so many girls cry and sigh at that time when i was just an male adolescent trying to understand women s behaviors in a small city in brazil this time however what caught my attention in the film was something very different namely the insistence with which the physician dr han suyin jennifer jones makes clear to the journalist mark elliott william holden her special ethically condition as an eurasian in fact she is constantly putting emphasis on this point in their relationship repeating she is willing to assume her love for him and carry it on in a occidental way provided that by doing so she is not betraying her chinese side its seems to the spectator that suyin is eagerly making efforts to establish a very subtle conciliation between those two unstable and opposite aspects of her culture for they will immediately engage in overt conflict in her mind at a minimum failure in her attempts to control them therefore suyin s attitudes always leave poor elliott a determined brave and extremely practical man anxious and perplexed without knowing how much importance to give to her words for him whose love for her is plain and simple the situation is totally clear if we love each other let us make a couple and begin immediately a life together not so fast is what she seems verbally and non verbally to answer him all the time in fact suyin s chinese portion would never allow her such a level of pragmatism and as she goes on and on reinforcing this much aimed equilibrium between those two worlds inside herself she also frequently signals to him that also a very peculiar trait of chinese culture is deeply rooted in her mind namely the constant raids on the real world by invisible beings from an spiritual or non physical world for suyin is always alerting elliott about how dangerous is life not because of any objective and concrete threat as would be the perpetuation of the english colonialism or the eminence of a japanese invasion but due to the threats of plenty of cruel and harmful gods and other mystical and mythical beings over the poor fearful and vulnerable human beings in fact it looks like a whole bunch of chinese deities are permanently on the watch to make people s life totally miserable because of that mothers must dress their precious male babies in girls clothes so that they are not taken away by jealous gods everyone should always be ready to make loud noises to send the clouds away in order to avoid their covering the sight of the moon peasants are advised that they should shout loudly the rice is bad the rice is bad to protect their crops from being stolen by deities and in a funeral it is recommended that the dead s family be isolated from the other people by curtains so that the gods don t take advantage of their sorrow and fragility in other words suyin introduces us to a culture in which the supernatural has a real existence as if a rather disturbing pantheon of malign and sadistic gods are always on the verge of negatively interfering with the most banal acts in anyone s daily life as the story takes place in hong kong in it should be clear that china really was at that time almost a semi feudal society while the country from which elliott had come from was not yet dominated by the fierce capitalism that launched by the usa after the first oil shock in took charge of the whole world therefore at least in one aspect both sides of suyin s eurasian personality were still much more innocent than they would be today a lot of history came into being since those old days as to china the main fact is that after several phases of a communist regime the country finally reached in the last two decades the condition of a very aggressive economy much more properly described as state capitalism and what happened to that old spirituality that so much enthralled suyin in hong kong in and with which she used to impress so much an impassioned elliott under that tree on the hill behind the hospital it is gone completely gone in brief if that story took place today elliott would not find it necessary to go to china to propose to suyin in the presence of the third uncle and her entire family in fact both men would now be incomparably closer to one another in their huge pragmatism talking business as usual\n",
      "\n",
      "i m disappointed that reiser who wrote the film felt the need to use so much profanity for no reason whatsoever maybe that s his idea of adult films plenty of nasty words with bathroom humor thrown in i thought better of him and think less of him for this movie falk s acting and some moments of humor as well as some possibly important themes are what made me give it such a high rating this might be a good movie for adult children to watch and laugh over about their own folks and their foibles but the lack of consideration for audience families seriously detriments what could have been a family film but fails certainly not worth spending money on though it might be worth a watch for free on television\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.apply(preprocess_text)\n",
    "X_test = X_test.apply(preprocess_text)\n",
    "\n",
    "for i in  selection:\n",
    "    print(X_train[i]+\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And the tokenizer ?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöß **Question** üöß\n",
    "\n",
    "How to do a simple \"White Space\" tokenizer ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'm',\n",
       " 'a',\n",
       " 'student',\n",
       " 'are',\n",
       " 'you',\n",
       " 'a',\n",
       " 'student',\n",
       " 'alex',\n",
       " 's',\n",
       " 'course',\n",
       " 'has',\n",
       " 'already',\n",
       " 'started']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try simple code \n",
    "# Test it on sample sentences. \n",
    "\n",
    "testsentence=\"I'm a student. Are you a student? Alex's course has already started!\"\n",
    "preprocess_text(testsentence).split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a more useful way, a tokenizer should be able to: \n",
    "- tokenize string\n",
    "- build a vocabulary\n",
    "- maybe keep track of word frequencies\n",
    "- build a natural text from its tokenized version.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/allauzen/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "class WhiteSpaceTokenizer:\n",
    "    def __init__(self):\n",
    "        # The vocabulary will store the mapping between text tokens and their id.\n",
    "        self.vocab = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "        # We will keep track of the number of times a word appears in the corpus.\n",
    "        self.frequencies = {}\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Converts the text to a list of tokens (substrings).\"\"\"\n",
    "        return word_tokenize(text)\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Take a text as input and return its associated tokenization, as a list of ids.\"\"\"\n",
    "        list_tokens = self.split_text(text)\n",
    "        list_ids = []\n",
    "        for token in list_tokens:\n",
    "            list_ids.append(self.vocab.get(token, -1))\n",
    "        return list_ids\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        tokens = [self.id_to_token[i] for i in ids]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def fit(self, corpus: List[str]):\n",
    "        \"\"\"Fits the tokenizer to a list of texts to construct its vocabulary.\"\"\"\n",
    "        current_id = 0\n",
    "        for text in corpus:\n",
    "            list_tokens = self.split_text(text)\n",
    "            for token in list_tokens:\n",
    "                token_id = self.vocab.get(token, None)\n",
    "                if token_id is None:\n",
    "                    self.vocab[token] = current_id\n",
    "                    self.id_to_token[current_id] = token\n",
    "                    self.frequencies[current_id] = 0\n",
    "                    token_id = current_id\n",
    "                    current_id += 1\n",
    "\n",
    "                self.frequencies[token_id] += 1\n",
    "        self.num_words = current_id\n",
    "        self.vocab[\"<UNK>\"] = -1\n",
    "        self.id_to_token[-1] = \"<UNK>\"\n",
    "        print(f\"Built a vocabulary of {self.num_words} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a vocabulary of 32830 words.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WhiteSpaceTokenizer()\n",
    "tokenizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 0,\n",
       " 'mean': 1,\n",
       " 'really': 2,\n",
       " 'this': 3,\n",
       " 'is': 4,\n",
       " 'not': 5,\n",
       " 'going': 6,\n",
       " 'to': 7,\n",
       " 'help': 8,\n",
       " 'the': 9,\n",
       " 'australian': 10,\n",
       " 'film': 11,\n",
       " 'industry': 12,\n",
       " 'make': 13,\n",
       " 'kind': 14,\n",
       " 'of': 15,\n",
       " 'with': 16,\n",
       " 'no': 17,\n",
       " 'values': 18,\n",
       " 'any': 19,\n",
       " 'okay': 20,\n",
       " 'if': 21,\n",
       " 'you': 22,\n",
       " 're': 23,\n",
       " 'a': 24,\n",
       " 'stoner': 25,\n",
       " 'and': 26,\n",
       " 'have': 27,\n",
       " 'nothing': 28,\n",
       " 'better': 29,\n",
       " 'do': 30,\n",
       " 'then': 31,\n",
       " 'maybe': 32,\n",
       " 'think': 33,\n",
       " 'makers': 34,\n",
       " 'from': 35,\n",
       " 'here': 36,\n",
       " 'should': 37,\n",
       " 'try': 38,\n",
       " 'show': 39,\n",
       " 'rest': 40,\n",
       " 'world': 41,\n",
       " 'what': 42,\n",
       " 'great': 43,\n",
       " 'talented': 44,\n",
       " 'people': 45,\n",
       " 'we': 46,\n",
       " 'vehicle': 47,\n",
       " 'for': 48,\n",
       " 'it': 49,\n",
       " 'come': 50,\n",
       " 'on': 51,\n",
       " 'now': 52,\n",
       " 'just': 53,\n",
       " 'tacky': 54,\n",
       " 'll': 55,\n",
       " 'give': 56,\n",
       " 'writer': 57,\n",
       " 'director': 58,\n",
       " 'william': 59,\n",
       " 'gove': 60,\n",
       " 'credit': 61,\n",
       " 'finding': 62,\n",
       " 'someone': 63,\n",
       " 'finance': 64,\n",
       " 'ill': 65,\n",
       " 'conceived': 66,\n",
       " 'thriller': 67,\n",
       " 'good': 68,\n",
       " 'argument': 69,\n",
       " 'wasting': 70,\n",
       " 'money': 71,\n",
       " 'subscribing': 72,\n",
       " 'hbo': 73,\n",
       " 'let': 74,\n",
       " 'alone': 75,\n",
       " 'buying': 76,\n",
       " 'dvds': 77,\n",
       " 'based': 78,\n",
       " 'cover': 79,\n",
       " 'art': 80,\n",
       " 'blurbs': 81,\n",
       " 'pedestrian': 82,\n",
       " 'dennis': 83,\n",
       " 'hopper': 84,\n",
       " 'game': 85,\n",
       " 'richard': 86,\n",
       " 'grieco': 87,\n",
       " 'add': 88,\n",
       " 'significant': 89,\n",
       " 'their': 90,\n",
       " 'resumes': 91,\n",
       " 'although': 92,\n",
       " 'direction': 93,\n",
       " 'half': 94,\n",
       " 'bad': 95,\n",
       " 'dialogue': 96,\n",
       " 'will': 97,\n",
       " 'leave': 98,\n",
       " 'grimacing': 99,\n",
       " 'wonder': 100,\n",
       " 'at': 101,\n",
       " 'its': 102,\n",
       " 'conceit': 103,\n",
       " 'storytelling': 104,\n",
       " 'worst': 105,\n",
       " 'tension': 106,\n",
       " 'suspense': 107,\n",
       " 'dread': 108,\n",
       " 'fear': 109,\n",
       " 'empathy': 110,\n",
       " 'catharsis': 111,\n",
       " 'few': 112,\n",
       " 'attractive': 113,\n",
       " 'often': 114,\n",
       " 'nude': 115,\n",
       " 'females': 116,\n",
       " 'spice': 117,\n",
       " 'up': 118,\n",
       " 'boredom': 119,\n",
       " 'but': 120,\n",
       " 'definitely': 121,\n",
       " 'best': 122,\n",
       " 'seen': 123,\n",
       " 'as': 124,\n",
       " 'trailer': 125,\n",
       " 'feel': 126,\n",
       " 'sorry': 127,\n",
       " 'guy': 128,\n",
       " 'who': 129,\n",
       " 'greenlighted': 130,\n",
       " 'thing': 131,\n",
       " 'late': 132,\n",
       " 'night': 133,\n",
       " 'zoned': 134,\n",
       " 'out': 135,\n",
       " 'viewing': 136,\n",
       " 'only': 137,\n",
       " 'been': 138,\n",
       " 'warned': 139,\n",
       " 'joseph': 140,\n",
       " 'brady': 141,\n",
       " 'clarence': 142,\n",
       " 'doolittle': 143,\n",
       " 'are': 144,\n",
       " 'two': 145,\n",
       " 'sailors': 146,\n",
       " 'four': 147,\n",
       " 'day': 148,\n",
       " 'shore': 149,\n",
       " 'in': 150,\n",
       " 'hollywood': 151,\n",
       " 'joe': 152,\n",
       " 'knows': 153,\n",
       " 'everything': 154,\n",
       " 'about': 155,\n",
       " 'girls': 156,\n",
       " 'can': 157,\n",
       " 't': 158,\n",
       " 'wait': 159,\n",
       " 'see': 160,\n",
       " 'lola': 161,\n",
       " 'while': 162,\n",
       " 'shyer': 163,\n",
       " 'needs': 164,\n",
       " 'some': 165,\n",
       " 'advice': 166,\n",
       " 'his': 167,\n",
       " 'buddy': 168,\n",
       " 'how': 169,\n",
       " 'meet': 170,\n",
       " 'they': 171,\n",
       " 'run': 172,\n",
       " 'into': 173,\n",
       " 'little': 174,\n",
       " 'boy': 175,\n",
       " 'donald': 176,\n",
       " 'martin': 177,\n",
       " 'has': 178,\n",
       " 'ran': 179,\n",
       " 'away': 180,\n",
       " 'order': 181,\n",
       " 'join': 182,\n",
       " 'navy': 183,\n",
       " 'take': 184,\n",
       " 'him': 185,\n",
       " 'home': 186,\n",
       " 'beautiful': 187,\n",
       " 'aunt': 188,\n",
       " 'susan': 189,\n",
       " 'wants': 190,\n",
       " 'be': 191,\n",
       " 'singer': 192,\n",
       " 'susie': 193,\n",
       " 'girl': 194,\n",
       " 'shyness': 195,\n",
       " 'gets': 196,\n",
       " 'way': 197,\n",
       " 'he': 198,\n",
       " 'doesn': 199,\n",
       " 'shy': 200,\n",
       " 'waitress': 201,\n",
       " 'comes': 202,\n",
       " 'brooklyn': 203,\n",
       " 'like': 204,\n",
       " 'does': 205,\n",
       " 'soon': 206,\n",
       " 'notices': 207,\n",
       " 's': 208,\n",
       " 'love': 209,\n",
       " 'boys': 210,\n",
       " 'fix': 211,\n",
       " 'when': 212,\n",
       " 'lie': 213,\n",
       " 'meeting': 214,\n",
       " 'big': 215,\n",
       " 'time': 216,\n",
       " 'music': 217,\n",
       " 'producer': 218,\n",
       " 'don': 219,\n",
       " 'even': 220,\n",
       " 'know': 221,\n",
       " 'feelings': 222,\n",
       " 'george': 223,\n",
       " 'sidney': 224,\n",
       " 'anchors': 225,\n",
       " 'aweigh': 226,\n",
       " 'musical': 227,\n",
       " 'comedy': 228,\n",
       " 'gene': 229,\n",
       " 'kelly': 230,\n",
       " 'top': 231,\n",
       " 'notch': 232,\n",
       " 'once': 233,\n",
       " 'again': 234,\n",
       " 'singing': 235,\n",
       " 'dancing': 236,\n",
       " 'routines': 237,\n",
       " 'frank': 238,\n",
       " 'sinatra': 239,\n",
       " 'terrific': 240,\n",
       " 'isn': 241,\n",
       " 'first': 242,\n",
       " 'that': 243,\n",
       " 'mind': 244,\n",
       " 'plays': 245,\n",
       " 'part': 246,\n",
       " 'well': 247,\n",
       " 'kathryn': 248,\n",
       " 'grayson': 249,\n",
       " 'fantastic': 250,\n",
       " 'abbott': 251,\n",
       " 'sadly': 252,\n",
       " 'lost': 253,\n",
       " 'gifted': 254,\n",
       " 'actress': 255,\n",
       " 'operatic': 256,\n",
       " 'soprano': 257,\n",
       " 'last': 258,\n",
       " 'month': 259,\n",
       " 'age': 260,\n",
       " 'year': 261,\n",
       " 'old': 262,\n",
       " 'dean': 263,\n",
       " 'stockwell': 264,\n",
       " 'amazing': 265,\n",
       " 'job': 266,\n",
       " 'fellow': 267,\n",
       " 'wanting': 268,\n",
       " 'become': 269,\n",
       " 'sailor': 270,\n",
       " 'jose': 271,\n",
       " 'iturbi': 272,\n",
       " 'performing': 273,\n",
       " 'himself': 274,\n",
       " 'magic': 275,\n",
       " 'piano': 276,\n",
       " 'edgar': 277,\n",
       " 'kennedy': 278,\n",
       " 'chief': 279,\n",
       " 'police': 280,\n",
       " 'station': 281,\n",
       " 'sara': 282,\n",
       " 'berner': 283,\n",
       " 'voice': 284,\n",
       " 'jerry': 285,\n",
       " 'mouse': 286,\n",
       " 'there': 287,\n",
       " 'lot': 288,\n",
       " 'stuff': 289,\n",
       " 'movie': 290,\n",
       " 'numbers': 291,\n",
       " 'look': 292,\n",
       " 'hate': 293,\n",
       " 'so': 294,\n",
       " 'energetic': 295,\n",
       " 'knew': 296,\n",
       " 'quite': 297,\n",
       " 'funny': 298,\n",
       " 'nice': 299,\n",
       " 'moment': 300,\n",
       " 'sings': 301,\n",
       " 'brahms': 302,\n",
       " 'lullaby': 303,\n",
       " 'lovely': 304,\n",
       " 'listen': 305,\n",
       " 'tango': 306,\n",
       " 'jealousy': 307,\n",
       " 'most': 308,\n",
       " 'memorable': 309,\n",
       " 'sequence': 310,\n",
       " 'one': 311,\n",
       " 'takes': 312,\n",
       " 'animated': 313,\n",
       " 'fantasy': 314,\n",
       " 'dances': 315,\n",
       " 'also': 316,\n",
       " 'tom': 317,\n",
       " 'cat': 318,\n",
       " 'butler': 319,\n",
       " 'originally': 320,\n",
       " 'asked': 321,\n",
       " 'mickey': 322,\n",
       " 'refused': 323,\n",
       " 'was': 324,\n",
       " 'nominated': 325,\n",
       " 'five': 326,\n",
       " 'oscars': 327,\n",
       " 'georgie': 328,\n",
       " 'stoll': 329,\n",
       " 'got': 330,\n",
       " 'original': 331,\n",
       " 'score': 332,\n",
       " 'high': 333,\n",
       " 'class': 334,\n",
       " 'entertainment': 335,\n",
       " 'trouble': 336,\n",
       " 'your': 337,\n",
       " 'watching': 338,\n",
       " 'numerous': 339,\n",
       " 'alternate': 340,\n",
       " 'titles': 341,\n",
       " 'generally': 342,\n",
       " 'means': 343,\n",
       " 'tried': 344,\n",
       " 'retried': 345,\n",
       " 'hide': 346,\n",
       " 'turkey': 347,\n",
       " 'various': 348,\n",
       " 'markets': 349,\n",
       " 'such': 350,\n",
       " 'brain': 351,\n",
       " 'machine': 352,\n",
       " 'which': 353,\n",
       " 'seven': 354,\n",
       " 'different': 355,\n",
       " 'super': 356,\n",
       " 'secret': 357,\n",
       " 'government': 358,\n",
       " 'project': 359,\n",
       " 'suppose': 360,\n",
       " 'able': 361,\n",
       " 'use': 362,\n",
       " 'computer': 363,\n",
       " 'read': 364,\n",
       " 'instead': 365,\n",
       " 'drives': 366,\n",
       " 'kill': 367,\n",
       " 'each': 368,\n",
       " 'other': 369,\n",
       " 'or': 370,\n",
       " 'themselves': 371,\n",
       " 'something': 372,\n",
       " 'filled': 373,\n",
       " 'b': 374,\n",
       " 'level': 375,\n",
       " 'tv': 376,\n",
       " 'actors': 377,\n",
       " 'sitting': 378,\n",
       " 'paneled': 379,\n",
       " 'room': 380,\n",
       " 'lawn': 381,\n",
       " 'chairs': 382,\n",
       " 'trying': 383,\n",
       " 'act': 384,\n",
       " 'script': 385,\n",
       " 'makes': 386,\n",
       " 'almost': 387,\n",
       " 'sense': 388,\n",
       " 'untastey': 389,\n",
       " 'avoid': 390,\n",
       " 'by': 391,\n",
       " 'far': 392,\n",
       " 'absolute': 393,\n",
       " 'years': 394,\n",
       " 'saw': 395,\n",
       " 'michael': 396,\n",
       " 'madsen': 397,\n",
       " 'figured': 398,\n",
       " 'couldn': 399,\n",
       " 'too': 400,\n",
       " 'since': 401,\n",
       " 'pretty': 402,\n",
       " 'decent': 403,\n",
       " 'films': 404,\n",
       " 'fair': 405,\n",
       " 'actor': 406,\n",
       " 'wrong': 407,\n",
       " 'waste': 408,\n",
       " 'fast': 409,\n",
       " 'forwarded': 410,\n",
       " 'through': 411,\n",
       " 'percent': 412,\n",
       " 'missed': 413,\n",
       " 'watch': 414,\n",
       " 'bond': 415,\n",
       " 'octopussy': 416,\n",
       " 'still': 417,\n",
       " 'enjoy': 418,\n",
       " 'accept': 419,\n",
       " 'production': 420,\n",
       " 'aren': 421,\n",
       " 'much': 422,\n",
       " 'above': 423,\n",
       " 'sean': 424,\n",
       " 'connery': 425,\n",
       " 'wrinkles': 426,\n",
       " 'forehead': 427,\n",
       " 'beneath': 428,\n",
       " 'an': 429,\n",
       " 'obvious': 430,\n",
       " 'toupee': 431,\n",
       " 'james': 432,\n",
       " 'get': 433,\n",
       " 'past': 434,\n",
       " 'inexperienced': 435,\n",
       " 'basinger': 436,\n",
       " 'weaker': 437,\n",
       " 'largo': 438,\n",
       " 'jolly': 439,\n",
       " 'q': 440,\n",
       " 'learn': 441,\n",
       " 'idiosyncratic': 442,\n",
       " 'barry': 443,\n",
       " 'believe': 444,\n",
       " 'hyperbolic': 445,\n",
       " 'reviews': 446,\n",
       " 'greeted': 447,\n",
       " 'release': 448,\n",
       " 'poker': 449,\n",
       " 'battle': 450,\n",
       " 'video': 451,\n",
       " 'face': 452,\n",
       " 'off': 453,\n",
       " 'them': 454,\n",
       " 'both': 455,\n",
       " 'same': 456,\n",
       " 'yours': 457,\n",
       " 'never': 458,\n",
       " 'say': 459,\n",
       " 'more': 460,\n",
       " 'probably': 461,\n",
       " 'my': 462,\n",
       " 'son': 463,\n",
       " 'yes': 464,\n",
       " 'howling': 465,\n",
       " 'classic': 466,\n",
       " 'rather': 467,\n",
       " 'werewolf': 468,\n",
       " 'admit': 469,\n",
       " 'started': 470,\n",
       " 'slowly': 471,\n",
       " 'gained': 472,\n",
       " 'momentum': 473,\n",
       " 'along': 474,\n",
       " 'finish': 475,\n",
       " 'anchorwoman': 476,\n",
       " 'changed': 477,\n",
       " 'cute': 478,\n",
       " 'gunned': 479,\n",
       " 'down': 480,\n",
       " 'camera': 481,\n",
       " 'made': 482,\n",
       " 'entertaining': 483,\n",
       " 'horror': 484,\n",
       " 'sure': 485,\n",
       " 'forget': 486,\n",
       " 'all': 487,\n",
       " 'oh': 488,\n",
       " 'anchor': 489,\n",
       " 'woman': 490,\n",
       " 'her': 491,\n",
       " 'brother': 492,\n",
       " 'find': 493,\n",
       " 'why': 494,\n",
       " 'things': 495,\n",
       " 'went': 496,\n",
       " 'did': 497,\n",
       " 'go': 498,\n",
       " 'cozy': 499,\n",
       " 'retreat': 500,\n",
       " 'transylvania': 501,\n",
       " 'somewhere': 502,\n",
       " 'where': 503,\n",
       " 'must': 504,\n",
       " 'evil': 505,\n",
       " 'magician': 506,\n",
       " 'werewolves': 507,\n",
       " 'christopher': 508,\n",
       " 'lee': 509,\n",
       " 'doing': 510,\n",
       " 'however': 511,\n",
       " 'trivia': 512,\n",
       " 'says': 513,\n",
       " 'had': 514,\n",
       " 'before': 515,\n",
       " 'role': 516,\n",
       " 'could': 517,\n",
       " 'gotten': 518,\n",
       " 'american': 519,\n",
       " 'london': 520,\n",
       " 'hell': 521,\n",
       " 'possible': 522,\n",
       " 'set': 523,\n",
       " 'after': 524,\n",
       " 'heck': 525,\n",
       " 'seem': 526,\n",
       " 'figure': 527,\n",
       " 'except': 528,\n",
       " 'bizarre': 529,\n",
       " 'prolonged': 530,\n",
       " 'sex': 531,\n",
       " 'scene': 532,\n",
       " 'fact': 533,\n",
       " 'death': 534,\n",
       " 'me': 535,\n",
       " 'gal': 536,\n",
       " 'talking': 537,\n",
       " 'loudly': 538,\n",
       " 'dude': 539,\n",
       " 'ear': 540,\n",
       " 'bleeding': 541,\n",
       " 'may': 542,\n",
       " 'am': 543,\n",
       " 'harsh': 544,\n",
       " 'absolutely': 545,\n",
       " 'stunned': 546,\n",
       " 'enough': 547,\n",
       " 'ask': 548,\n",
       " 'greatly': 549,\n",
       " 'exaggerated': 550,\n",
       " 'silly': 551,\n",
       " 'despite': 552,\n",
       " 'creepy': 553,\n",
       " 'scenes': 554,\n",
       " 'seriously': 555,\n",
       " 'ass': 556,\n",
       " 'stupid': 557,\n",
       " 'story': 558,\n",
       " 'actually': 559,\n",
       " 'deep': 560,\n",
       " 'investigating': 561,\n",
       " 'kayako': 562,\n",
       " 'found': 563,\n",
       " 'she': 564,\n",
       " 'mother': 565,\n",
       " 'miraculously': 566,\n",
       " 'speaks': 567,\n",
       " 'english': 568,\n",
       " 'exorcist': 569,\n",
       " 'fed': 570,\n",
       " 'spirits': 571,\n",
       " 'daughter': 572,\n",
       " 'yeap': 573,\n",
       " 'ok': 574,\n",
       " 'ordinary': 575,\n",
       " 'housewife': 576,\n",
       " 'affair': 577,\n",
       " 'bloke': 578,\n",
       " 'herself': 579,\n",
       " 'dead': 580,\n",
       " 'because': 581,\n",
       " 'rage': 582,\n",
       " 'became': 583,\n",
       " 'vengeful': 584,\n",
       " 'spirit': 585,\n",
       " 'kills': 586,\n",
       " 'anyone': 587,\n",
       " 'enters': 588,\n",
       " 'house': 589,\n",
       " 'acceotable': 590,\n",
       " 'killings': 591,\n",
       " 'began': 592,\n",
       " 'stretch': 593,\n",
       " 'opportunity': 594,\n",
       " 'travel': 595,\n",
       " 'throughout': 596,\n",
       " 'tokyo': 597,\n",
       " 'victims': 598,\n",
       " 'were': 599,\n",
       " 'travelling': 600,\n",
       " 'weren': 601,\n",
       " 'struck': 602,\n",
       " 'hard': 603,\n",
       " 'ghost': 604,\n",
       " 'country': 605,\n",
       " 'without': 606,\n",
       " 'paying': 607,\n",
       " 'public': 608,\n",
       " 'transport': 609,\n",
       " 'fares': 610,\n",
       " 'wouldn': 611,\n",
       " 'being': 612,\n",
       " 'snorts': 613,\n",
       " 'crown': 614,\n",
       " 'depicted': 615,\n",
       " 'very': 616,\n",
       " 'ju': 617,\n",
       " 'grudge': 618,\n",
       " 'than': 619,\n",
       " 'trash': 620,\n",
       " 'spectre': 621,\n",
       " 'truly': 622,\n",
       " 'enjoys': 623,\n",
       " 'felt': 624,\n",
       " 'mission': 625,\n",
       " 'worse': 626,\n",
       " 'viewer': 627,\n",
       " 'coming': 628,\n",
       " 'forms': 629,\n",
       " 'large': 630,\n",
       " 'strands': 631,\n",
       " 'hair': 632,\n",
       " 'power': 633,\n",
       " 'dun': 634,\n",
       " 'liked': 635,\n",
       " 'movies': 636,\n",
       " 'depicting': 637,\n",
       " 'ghosts': 638,\n",
       " 'monsters': 639,\n",
       " 'cause': 640,\n",
       " 'overall': 641,\n",
       " 'results': 642,\n",
       " 'plain': 643,\n",
       " 'storyline': 644,\n",
       " 'less': 645,\n",
       " 'exaggeration': 646,\n",
       " 'would': 647,\n",
       " 'oppenheimer': 648,\n",
       " 'brilliant': 649,\n",
       " 'series': 650,\n",
       " 'finest': 651,\n",
       " 'offerings': 652,\n",
       " 'ever': 653,\n",
       " 'pbs': 654,\n",
       " 'david': 655,\n",
       " 'suchet': 656,\n",
       " 'particularly': 657,\n",
       " 'effective': 658,\n",
       " 'edward': 659,\n",
       " 'teller': 660,\n",
       " 'recall': 661,\n",
       " 'conception': 662,\n",
       " 'spectacularly': 663,\n",
       " 'reason': 664,\n",
       " 'rate': 665,\n",
       " 'full': 666,\n",
       " 'low': 667,\n",
       " 'budget': 668,\n",
       " 'areas': 669,\n",
       " 'actual': 670,\n",
       " 'content': 671,\n",
       " 'recollection': 672,\n",
       " 'miniseries': 673,\n",
       " 'released': 674,\n",
       " 'uk': 675,\n",
       " 'july': 676,\n",
       " 'st': 677,\n",
       " 'region': 678,\n",
       " 'pal': 679,\n",
       " 'ntsc': 680,\n",
       " 'offing': 681,\n",
       " 'universal': 682,\n",
       " 'player': 683,\n",
       " 'us': 684,\n",
       " 'right': 685,\n",
       " 'amazon': 686,\n",
       " 'http': 687,\n",
       " 'tinyurl': 688,\n",
       " 'com': 689,\n",
       " 'znyyq': 690,\n",
       " 'huzzah': 691,\n",
       " 'rating': 692,\n",
       " 'mini': 693,\n",
       " 'approached': 694,\n",
       " 'aware': 695,\n",
       " 'six': 696,\n",
       " 'months': 697,\n",
       " 'sci': 698,\n",
       " 'fi': 699,\n",
       " 'channel': 700,\n",
       " 'continued': 701,\n",
       " 'pepper': 702,\n",
       " 'shows': 703,\n",
       " 'bg': 704,\n",
       " 'ads': 705,\n",
       " 'confess': 706,\n",
       " 'growing': 707,\n",
       " 'unease': 708,\n",
       " 'learned': 709,\n",
       " 'work': 710,\n",
       " 'cinematic': 711,\n",
       " 'stood': 712,\n",
       " 'test': 713,\n",
       " 'regard': 714,\n",
       " 'battlestar': 715,\n",
       " 'galactica': 716,\n",
       " 'remember': 717,\n",
       " 'chromium': 718,\n",
       " 'warriors': 719,\n",
       " 'oscillating': 720,\n",
       " 'red': 721,\n",
       " 'light': 722,\n",
       " 'visor': 723,\n",
       " 'others': 724,\n",
       " 'fondness': 725,\n",
       " 'held': 726,\n",
       " 'special': 727,\n",
       " 'effects': 728,\n",
       " 'evolutionary': 729,\n",
       " 'many': 730,\n",
       " 'state': 731,\n",
       " 'during': 732,\n",
       " 'especially': 733,\n",
       " 'those': 734,\n",
       " 'television': 735,\n",
       " 'memories': 736,\n",
       " 'resolve': 737,\n",
       " 'around': 738,\n",
       " 'arc': 739,\n",
       " 'relationships': 740,\n",
       " 'helped': 741,\n",
       " 'overcome': 742,\n",
       " 'challenges': 743,\n",
       " 'faced': 744,\n",
       " 'frankly': 745,\n",
       " 'latter': 746,\n",
       " 'group': 747,\n",
       " 'core': 748,\n",
       " 'pulled': 749,\n",
       " 'together': 750,\n",
       " 'save': 751,\n",
       " 'another': 752,\n",
       " 'empire': 753,\n",
       " 'cylons': 754,\n",
       " 'gain': 755,\n",
       " 'extermination': 756,\n",
       " 'human': 757,\n",
       " 'race': 758,\n",
       " 'yet': 759,\n",
       " 'base': 760,\n",
       " 'stars': 761,\n",
       " 'swirling': 762,\n",
       " 'men': 763,\n",
       " 'women': 764,\n",
       " 'came': 765,\n",
       " 'enemy': 766,\n",
       " 'virtually': 767,\n",
       " 'unlimited': 768,\n",
       " 'resources': 769,\n",
       " 'somehow': 770,\n",
       " 'managed': 771,\n",
       " 'survive': 772,\n",
       " 'until': 773,\n",
       " 'next': 774,\n",
       " 'didn': 775,\n",
       " 'technology': 776,\n",
       " 'fire': 777,\n",
       " 'survived': 778,\n",
       " 'cared': 779,\n",
       " 'trusted': 780,\n",
       " 'flaws': 781,\n",
       " 'times': 782,\n",
       " 'sappy': 783,\n",
       " 'care': 784,\n",
       " 'writers': 785,\n",
       " 'current': 786,\n",
       " 'rendition': 787,\n",
       " 'seemed': 788,\n",
       " 'understand': 789,\n",
       " 'ways': 790,\n",
       " 'took': 791,\n",
       " 'least': 792,\n",
       " 'character': 793,\n",
       " 'names': 794,\n",
       " 'crafted': 795,\n",
       " 'called': 796,\n",
       " 'reinvention': 797,\n",
       " 'science': 798,\n",
       " 'fiction': 799,\n",
       " 'goal': 800,\n",
       " 'judged': 801,\n",
       " 'accomplished': 802,\n",
       " 'failure': 803,\n",
       " 'derivitive': 804,\n",
       " 'endeavors': 805,\n",
       " 'long': 806,\n",
       " 'borrows': 807,\n",
       " 'liberally': 808,\n",
       " 'tng': 809,\n",
       " 'ds': 810,\n",
       " 'babylon': 811,\n",
       " 'battlefield': 812,\n",
       " 'earth': 813,\n",
       " 'unfortunate': 814,\n",
       " 'ronald': 815,\n",
       " 'd': 816,\n",
       " 'moore': 817,\n",
       " 'contributor': 818,\n",
       " 'popular': 819,\n",
       " 'decade': 820,\n",
       " 'contribution': 821,\n",
       " 'hope': 822,\n",
       " 'difficulties': 823,\n",
       " 'appears': 824,\n",
       " 'conflict': 825,\n",
       " 'bridge': 826,\n",
       " 'crew': 827,\n",
       " 'enterprise': 828,\n",
       " 'e': 829,\n",
       " 'inviolable': 830,\n",
       " 'rule': 831,\n",
       " 'roddenberry': 832,\n",
       " 'lived': 833,\n",
       " 'under': 834,\n",
       " 'rules': 835,\n",
       " 'every': 836,\n",
       " 'break': 837,\n",
       " 'longer': 838,\n",
       " 'authority': 839,\n",
       " 'ron': 840,\n",
       " 'seems': 841,\n",
       " 'forgotten': 842,\n",
       " 'lessons': 843,\n",
       " 'acknowledged': 844,\n",
       " 'master': 845,\n",
       " 'writing': 846,\n",
       " 'created': 847,\n",
       " 'dysfuntional': 848,\n",
       " 'cast': 849,\n",
       " 'intent': 850,\n",
       " 'creating': 851,\n",
       " 'besides': 852,\n",
       " 'dysfunctional': 853,\n",
       " 'bit': 854,\n",
       " 'believable': 855,\n",
       " 'military': 856,\n",
       " 'unprovokedly': 857,\n",
       " 'striking': 858,\n",
       " 'superior': 859,\n",
       " 'officer': 860,\n",
       " 'couple': 861,\n",
       " 'days': 862,\n",
       " 'hack': 863,\n",
       " 'execution': 864,\n",
       " 'happened': 865,\n",
       " 'period': 866,\n",
       " 'war': 867,\n",
       " 'remembered': 868,\n",
       " 'earlier': 869,\n",
       " 'penned': 870,\n",
       " 'capt': 871,\n",
       " 'kirk': 872,\n",
       " 'killed': 873,\n",
       " 'alas': 874,\n",
       " 'surfing': 875,\n",
       " 'infomercials': 876,\n",
       " 'sitcoms': 877,\n",
       " 'morning': 878,\n",
       " 'expect': 879,\n",
       " 'rose': 880,\n",
       " 'mcgowan': 881,\n",
       " 'hot': 882,\n",
       " 'performance': 883,\n",
       " 'oscar': 884,\n",
       " 'worthy': 885,\n",
       " 'ups': 886,\n",
       " 'downs': 887,\n",
       " 'twists': 888,\n",
       " 'end': 889,\n",
       " 'honesty': 890,\n",
       " 'awful': 891,\n",
       " 'typical': 892,\n",
       " 'slasher': 893,\n",
       " 'gore': 894,\n",
       " 'nudity': 895,\n",
       " 'real': 896,\n",
       " 'violence': 897,\n",
       " 'acting': 898,\n",
       " 'nightkill': 899,\n",
       " 'robert': 900,\n",
       " 'mitchum': 901,\n",
       " 'weary': 902,\n",
       " 'private': 903,\n",
       " 'eye': 904,\n",
       " 'probing': 905,\n",
       " 'case': 906,\n",
       " 'missing': 907,\n",
       " 'industrialist': 908,\n",
       " 'mike': 909,\n",
       " 'connors': 910,\n",
       " 'hired': 911,\n",
       " 'jaclyn': 912,\n",
       " 'smith': 913,\n",
       " 'anxious': 914,\n",
       " 'wife': 915,\n",
       " 'man': 916,\n",
       " 'fails': 917,\n",
       " 'inform': 918,\n",
       " 'husband': 919,\n",
       " 'whereabouts': 920,\n",
       " 'lover': 921,\n",
       " 'franciscus': 922,\n",
       " 'dispose': 923,\n",
       " 'wealthy': 924,\n",
       " 'hubby': 925,\n",
       " 'rotten': 926,\n",
       " 'mannix': 927,\n",
       " 'goes': 928,\n",
       " 'western': 929,\n",
       " 'monkeys': 930,\n",
       " 'abused': 931,\n",
       " 'models': 932,\n",
       " 'lean': 933,\n",
       " 'against': 934,\n",
       " 'cars': 935,\n",
       " 'constantly': 936,\n",
       " 'upstaged': 937,\n",
       " 'sybil': 938,\n",
       " 'danning': 939,\n",
       " 'giallo': 940,\n",
       " 'style': 941,\n",
       " 'wrap': 942,\n",
       " 'brings': 943,\n",
       " 'whole': 944,\n",
       " 'mess': 945,\n",
       " 'bitter': 946,\n",
       " 'cinema': 947,\n",
       " 'sooooo': 948,\n",
       " 'poor': 949,\n",
       " 'halloween': 950,\n",
       " 'mixed': 951,\n",
       " 'trick': 952,\n",
       " 'treats': 953,\n",
       " 'rated': 954,\n",
       " 'r': 955,\n",
       " 'graphic': 956,\n",
       " 'sexual': 957,\n",
       " 'situations': 958,\n",
       " 'among': 959,\n",
       " 'flock': 960,\n",
       " 'revenge': 961,\n",
       " 'minded': 962,\n",
       " 'action': 963,\n",
       " 'flicks': 964,\n",
       " 'bill': 965,\n",
       " 'vol': 966,\n",
       " 'punisher': 967,\n",
       " 'walking': 968,\n",
       " 'tall': 969,\n",
       " 'works': 970,\n",
       " 'thanks': 971,\n",
       " 'always': 972,\n",
       " 'watchable': 973,\n",
       " 'denzel': 974,\n",
       " 'washington': 975,\n",
       " 'today': 976,\n",
       " 'j': 977,\n",
       " 'quinnell': 978,\n",
       " 'novel': 979,\n",
       " 'filmed': 980,\n",
       " 'scott': 981,\n",
       " 'glenn': 982,\n",
       " 'luck': 983,\n",
       " 'ex': 984,\n",
       " 'mercenary': 985,\n",
       " 'stooped': 986,\n",
       " 'drinking': 987,\n",
       " 'flash': 988,\n",
       " 'jack': 989,\n",
       " 'daniels': 990,\n",
       " 'partner': 991,\n",
       " 'walken': 992,\n",
       " 'offers': 993,\n",
       " 'chance': 994,\n",
       " 'redemption': 995,\n",
       " 'bodyguard': 996,\n",
       " 'dakota': 997,\n",
       " 'fanning': 998,\n",
       " 'mexican': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.frequencies[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['after', 'many', 'many', 'years', 'i', 'saw', 'again', 'this', 'beautiful', 'love', 'story', 'thinking', 'about', 'how', 'would', 'i', 'half', 'a', 'century', 'after', 'react', 'to', 'a', 'film', 'which', 'made', 'so', 'many', 'girls', 'cry', 'and', 'sigh', 'at', 'that', 'time', 'when', 'i', 'was', 'just', 'an', 'male', 'adolescent', 'trying', 'to', 'understand', 'women', 's', 'behaviors', 'in', 'a', 'small', 'city', 'in', 'brazil', 'this', 'time', 'however', 'what', 'caught', 'my', 'attention', 'in', 'the', 'film', 'was', 'something', 'very', 'different', 'namely', 'the', 'insistence', 'with', 'which', 'the', 'physician', 'dr', 'han', 'suyin', 'jennifer', 'jones', 'makes', 'clear', 'to', 'the', 'journalist', 'mark', 'elliott', 'william', 'holden', 'her', 'special', 'ethically', 'condition', 'as', 'an', 'eurasian', 'in', 'fact', 'she', 'is', 'constantly', 'putting', 'emphasis', 'on', 'this', 'point', 'in', 'their', 'relationship', 'repeating', 'she', 'is', 'willing', 'to', 'assume', 'her', 'love', 'for', 'him', 'and', 'carry', 'it', 'on', 'in', 'a', 'occidental', 'way', 'provided', 'that', 'by', 'doing', 'so', 'she', 'is', 'not', 'betraying', 'her', 'chinese', 'side', 'its', 'seems', 'to', 'the', 'spectator', 'that', 'suyin', 'is', 'eagerly', 'making', 'efforts', 'to', 'establish', 'a', 'very', 'subtle', 'conciliation', 'between', 'those', 'two', 'unstable', 'and', 'opposite', 'aspects', 'of', 'her', 'culture', 'for', 'they', 'will', 'immediately', 'engage', 'in', 'overt', 'conflict', 'in', 'her', 'mind', 'at', 'a', 'minimum', 'failure', 'in', 'her', 'attempts', 'to', 'control', 'them', 'therefore', 'suyin', 's', 'attitudes', 'always', 'leave', 'poor', 'elliott', 'a', 'determined', 'brave', 'and', 'extremely', 'practical', 'man', 'anxious', 'and', 'perplexed', 'without', 'knowing', 'how', 'much', 'importance', 'to', 'give', 'to', 'her', 'words', 'for', 'him', 'whose', 'love', 'for', 'her', 'is', 'plain', 'and', 'simple', 'the', 'situation', 'is', 'totally', 'clear', 'if', 'we', 'love', 'each', 'other', 'let', 'us', 'make', 'a', 'couple', 'and', 'begin', 'immediately', 'a', 'life', 'together', 'not', 'so', 'fast', 'is', 'what', 'she', 'seems', 'verbally', 'and', 'non', 'verbally', 'to', 'answer', 'him', 'all', 'the', 'time', 'in', 'fact', 'suyin', 's', 'chinese', 'portion', 'would', 'never', 'allow', 'her', 'such', 'a', 'level', 'of', 'pragmatism', 'and', 'as', 'she', 'goes', 'on', 'and', 'on', 'reinforcing', 'this', 'much', 'aimed', 'equilibrium', 'between', 'those', 'two', 'worlds', 'inside', 'herself', 'she', 'also', 'frequently', 'signals', 'to', 'him', 'that', 'also', 'a', 'very', 'peculiar', 'trait', 'of', 'chinese', 'culture', 'is', 'deeply', 'rooted', 'in', 'her', 'mind', 'namely', 'the', 'constant', 'raids', 'on', 'the', 'real', 'world', 'by', 'invisible', 'beings', 'from', 'an', 'spiritual', 'or', 'non', 'physical', 'world', 'for', 'suyin', 'is', 'always', 'alerting', 'elliott', 'about', 'how', 'dangerous', 'is', 'life', 'not', 'because', 'of', 'any', 'objective', 'and', 'concrete', 'threat', 'as', 'would', 'be', 'the', 'perpetuation', 'of', 'the', 'english', 'colonialism', 'or', 'the', 'eminence', 'of', 'a', 'japanese', 'invasion', 'but', 'due', 'to', 'the', 'threats', 'of', 'plenty', 'of', 'cruel', 'and', 'harmful', 'gods', 'and', 'other', 'mystical', 'and', 'mythical', 'beings', 'over', 'the', 'poor', 'fearful', 'and', 'vulnerable', 'human', 'beings', 'in', 'fact', 'it', 'looks', 'like', 'a', 'whole', 'bunch', 'of', 'chinese', 'deities', 'are', 'permanently', 'on', 'the', 'watch', 'to', 'make', 'people', 's', 'life', 'totally', 'miserable', 'because', 'of', 'that', 'mothers', 'must', 'dress', 'their', 'precious', 'male', 'babies', 'in', 'girls', 'clothes', 'so', 'that', 'they', 'are', 'not', 'taken', 'away', 'by', 'jealous', 'gods', 'everyone', 'should', 'always', 'be', 'ready', 'to', 'make', 'loud', 'noises', 'to', 'send', 'the', 'clouds', 'away', 'in', 'order', 'to', 'avoid', 'their', 'covering', 'the', 'sight', 'of', 'the', 'moon', 'peasants', 'are', 'advised', 'that', 'they', 'should', 'shout', 'loudly', 'the', 'rice', 'is', 'bad', 'the', 'rice', 'is', 'bad', 'to', 'protect', 'their', 'crops', 'from', 'being', 'stolen', 'by', 'deities', 'and', 'in', 'a', 'funeral', 'it', 'is', 'recommended', 'that', 'the', 'dead', 's', 'family', 'be', 'isolated', 'from', 'the', 'other', 'people', 'by', 'curtains', 'so', 'that', 'the', 'gods', 'don', 't', 'take', 'advantage', 'of', 'their', 'sorrow', 'and', 'fragility', 'in', 'other', 'words', 'suyin', 'introduces', 'us', 'to', 'a', 'culture', 'in', 'which', 'the', 'supernatural', 'has', 'a', 'real', 'existence', 'as', 'if', 'a', 'rather', 'disturbing', 'pantheon', 'of', 'malign', 'and', 'sadistic', 'gods', 'are', 'always', 'on', 'the', 'verge', 'of', 'negatively', 'interfering', 'with', 'the', 'most', 'banal', 'acts', 'in', 'anyone', 's', 'daily', 'life', 'as', 'the', 'story', 'takes', 'place', 'in', 'hong', 'kong', 'in', 'it', 'should', 'be', 'clear', 'that', 'china', 'really', 'was', 'at', 'that', 'time', 'almost', 'a', 'semi', 'feudal', 'society', 'while', 'the', 'country', 'from', 'which', 'elliott', 'had', 'come', 'from', 'was', 'not', 'yet', 'dominated', 'by', 'the', 'fierce', 'capitalism', 'that', 'launched', 'by', 'the', 'usa', 'after', 'the', 'first', 'oil', 'shock', 'in', 'took', 'charge', 'of', 'the', 'whole', 'world', 'therefore', 'at', 'least', 'in', 'one', 'aspect', 'both', 'sides', 'of', 'suyin', 's', 'eurasian', 'personality', 'were', 'still', 'much', 'more', 'innocent', 'than', 'they', 'would', 'be', 'today', 'a', 'lot', 'of', 'history', 'came', 'into', 'being', 'since', 'those', 'old', 'days', 'as', 'to', 'china', 'the', 'main', 'fact', 'is', 'that', 'after', 'several', 'phases', 'of', 'a', 'communist', 'regime', 'the', 'country', 'finally', 'reached', 'in', 'the', 'last', 'two', 'decades', 'the', 'condition', 'of', 'a', 'very', 'aggressive', 'economy', 'much', 'more', 'properly', 'described', 'as', 'state', 'capitalism', 'and', 'what', 'happened', 'to', 'that', 'old', 'spirituality', 'that', 'so', 'much', 'enthralled', 'suyin', 'in', 'hong', 'kong', 'in', 'and', 'with', 'which', 'she', 'used', 'to', 'impress', 'so', 'much', 'an', 'impassioned', 'elliott', 'under', 'that', 'tree', 'on', 'the', 'hill', 'behind', 'the', 'hospital', 'it', 'is', 'gone', 'completely', 'gone', 'in', 'brief', 'if', 'that', 'story', 'took', 'place', 'today', 'elliott', 'would', 'not', 'find', 'it', 'necessary', 'to', 'go', 'to', 'china', 'to', 'propose', 'to', 'suyin', 'in', 'the', 'presence', 'of', 'the', 'third', 'uncle', 'and', 'her', 'entire', 'family', 'in', 'fact', 'both', 'men', 'would', 'now', 'be', 'incomparably', 'closer', 'to', 'one', 'another', 'in', 'their', 'huge', 'pragmatism', 'talking', 'business', 'as', 'usual']\n",
      "['i', 'm', 'disappointed', 'that', 'reiser', 'who', 'wrote', 'the', 'film', 'felt', 'the', 'need', 'to', 'use', 'so', 'much', 'profanity', 'for', 'no', 'reason', 'whatsoever', 'maybe', 'that', 's', 'his', 'idea', 'of', 'adult', 'films', 'plenty', 'of', 'nasty', 'words', 'with', 'bathroom', 'humor', 'thrown', 'in', 'i', 'thought', 'better', 'of', 'him', 'and', 'think', 'less', 'of', 'him', 'for', 'this', 'movie', 'falk', 's', 'acting', 'and', 'some', 'moments', 'of', 'humor', 'as', 'well', 'as', 'some', 'possibly', 'important', 'themes', 'are', 'what', 'made', 'me', 'give', 'it', 'such', 'a', 'high', 'rating', 'this', 'might', 'be', 'a', 'good', 'movie', 'for', 'adult', 'children', 'to', 'watch', 'and', 'laugh', 'over', 'about', 'their', 'own', 'folks', 'and', 'their', 'foibles', 'but', 'the', 'lack', 'of', 'consideration', 'for', 'audience', 'families', 'seriously', 'detriments', 'what', 'could', 'have', 'been', 'a', 'family', 'film', 'but', 'fails', 'certainly', 'not', 'worth', 'spending', 'money', 'on', 'though', 'it', 'might', 'be', 'worth', 'a', 'watch', 'for', 'free', 'on', 'television']\n"
     ]
    }
   ],
   "source": [
    "for i in  selection:\n",
    "    print(tokenizer.split_text(X_train[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#¬†Basic Text classifiers\n",
    "\n",
    "Now wa have pre-processed and tokenized text ! Let's go for classification. To allow fair and easy comparison we start by a Base class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTextClassifier:\n",
    "\n",
    "    def predict(self, text: str) -> int:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict_dataset(self, X: List[str]) -> List[int]:\n",
    "        return [self.predict(x) for x in X]\n",
    "\n",
    "    def evaluate(self, X: List[str], y: List[int]) -> Dict[str, float]:\n",
    "        predictions = self.predict_dataset(X)\n",
    "        y_array = np.array(y)\n",
    "        accuracy = np.mean(predictions == y_array)\n",
    "        return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöß **Question** üöß\n",
    "Propose a a very basic classifier. \n",
    "- Gather counts of \"positive\" and \"negative\" words.\n",
    "- But how do we know if a word is positive or negative ?\n",
    "- We can look at words in the training set.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Inherit from the Base class and implement a classifier based on counts \n",
    "\n",
    "class CountBasedClassifier( BaseTextClassifier ): \n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        self.tokenizer = WhiteSpaceTokenizer()\n",
    "        self.counts = {}\n",
    "        \n",
    "    def fit(self, X, y): \n",
    "        self.tokenizer.fit(X)\n",
    "        #¬†X est le corpus : un ensemble de textes\n",
    "        for i in range(len(X)):\n",
    "            tokens = self.tokenizer.encode(X[i])\n",
    "            #¬†tokens la liste des mots (sous forme d'indices) dans le texte X[i]\n",
    "            for iw in tokens: \n",
    "                if iw == -1: \n",
    "                    continue \n",
    "                key =  y[i],iw\n",
    "                if key in self.counts.keys():               \n",
    "                    self.counts[key] += 1 \n",
    "                else :\n",
    "                    self.counts[key] = 1 \n",
    "                    \n",
    "    def predict(self, text): \n",
    "        wordidx = self.tokenizer.encode(text)\n",
    "        #¬†scores par classe pour le text\n",
    "        scores = np.zeros(self.n_classes) \n",
    "        for i in wordidx :\n",
    "            if i == -1 :\n",
    "                continue\n",
    "            for c in range(self.n_classes): \n",
    "                key = c,i\n",
    "                if key in self.counts.keys(): \n",
    "                    scores[c] += self.counts[c,i]\n",
    "        return np.argmax(scores)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a vocabulary of 32830 words.\n"
     ]
    }
   ],
   "source": [
    "classif = CountBasedClassifier(2)\n",
    "classif.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif.predict(\"I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': np.float64(0.6506666666666666)}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy \")\n",
    "classif.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equilibre entre classe:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.49333333333333335)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Equilibre entre classe:\")\n",
    "y_test.sum()/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you know the Zipf Law ? \n",
    "\n",
    "üöß **Question** üöß\n",
    "The tokenizer contains the counts for every words in the training set. Plot the the word counts in descending order. \n",
    "Comment the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wc = list(classif.tokenizer.frequencies.values())\n",
    "wc.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x775c6bf67740>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.loglog(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Inherit from the Base class and implement a classifier based on counts \n",
    "\n",
    "class ProbaClassifier( BaseTextClassifier ): \n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        # This attribute is not used, but kept for API consistency\n",
    "        self.tokenizer = WhiteSpaceTokenizer()\n",
    "\n",
    "    def get_word_count(self, X, y):\n",
    "        words_count = np.zeros((self.n_classes, self.tokenizer.num_words), dtype=int)\n",
    "        for i in range(len(X)):\n",
    "            tokens = self.tokenizer.encode(X[i])\n",
    "            for w in tokens:\n",
    "                if w != -1:\n",
    "                    words_count[y[i], w] += 1\n",
    "        frequencies = words_count / words_count.sum(axis=1, keepdims=True)\n",
    "        return words_count, frequencies\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tokenizer.fit(X)\n",
    "        word_count, frequencies = self.get_word_count(X, y)\n",
    "        self.frequencies = frequencies\n",
    "        self.words_count = word_count\n",
    "\n",
    "    def predict(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Calculate a score for each label by summing\n",
    "        the normalized frequencies of the words in 'text'.\n",
    "        Then choose the label with the highest score.\n",
    "        \"\"\"\n",
    "        global_count = self.words_count.sum(axis=0)\n",
    "        global_freq = global_count / global_count.sum()\n",
    "        words = self.tokenizer.encode(text)\n",
    "        words = np.array([w for w in words if w != -1])\n",
    "        words_count_per_class = self.frequencies[:, words] / global_freq[words]\n",
    "        scores = words_count_per_class.sum(axis=1)\n",
    "        return np.argmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a vocabulary of 32830 words.\n",
      "{'accuracy': np.float64(0.8193333333333334)}\n"
     ]
    }
   ],
   "source": [
    "classifier = ProbaClassifier(2)\n",
    "classifier.fit(X_train, y_train)\n",
    "print(classifier.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/allauzen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/allauzen/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flower\n",
      "flower\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "lem = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "print(lem.lemmatize(\"flowers\"))\n",
    "print(porter.stem(\"flowers\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you know SciKit-Learn ? \n",
    "\n",
    "üöß **Question** üöß\n",
    "\n",
    "How can we optimize the algorithm? \n",
    "- For the tokenization and/or vectorization ?\n",
    "- For the classifier ? Write you count-based classifier that uses the vectorizer of SKLearn.\n",
    "-  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "Xvtrain = vectorizer.fit_transform(X_train).toarray()\n",
    "Xvtest = vectorizer.transform(X_test).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a \"SKLearn\" dataset,  we can also use all the classifiers we want from this library: \n",
    "- Naive Bayes\n",
    "- Logistic Regression\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vectorization and NLP\n",
    "\n",
    "In Machine Learning for supervised classification, many approaches work with vectors. The input to be classified is represented by a vector which is given to the classifier. This input vector gathers a set of numerical features that describes the input. What kind of features can we extract from text ?  \n",
    "The \"easy features\" are what we have done: \n",
    "- Counting words (do we need all the words ?)\n",
    "- Could we use binary features ? \n",
    "- Counting n-grams (why ?) \n",
    "\n",
    "\n",
    "üöß **Question** üöß\n",
    "\n",
    "For instance, take what we did for the logistic regression and provide an interpretation of the model, its decision rule and the parameters ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag of words model\n",
    "\n",
    "- We count the number of times each word appears in a text.\n",
    "- We can then represent the text as a vector of counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frequency-based vectorization\n",
    " \n",
    "- We can normalize the counts by the total number of words in the text.\n",
    "- This gives us the frequency of each word in the text.\n",
    "- This is called the **term frequency**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TF-IDF\n",
    "\n",
    "- **Intuition**:\n",
    "  - If a word appears many times in a document, it is important.\n",
    "  - If a word appears in many documents, it is not very informative.\n",
    "  - For instance, the words \"the\", \"and\" appears in many documents.\n",
    "\n",
    "- **TF-IDF**:\n",
    "  - We multiply the term frequency by the inverse document frequency.\n",
    "  - Used a lot for information retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term Frequency (TF)\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{f_{t, d}}{\\sum_{t' \\in d} f_{t', d}}\n",
    "$$\n",
    "Where:\n",
    "- $ f_{t, d} $: Frequency of term $ t $ in document $ d $.\n",
    "- $ \\sum_{t' \\in d} f_{t', d} $: Total number of terms in document $ d $.\n",
    "\n",
    "---\n",
    "\n",
    "### Inverse Document Frequency (IDF)\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{N + 1}{\\text{DF}(t) + 1}\\right) + 1\n",
    "$$\n",
    "Where:\n",
    "- $ N $: Total number of documents.\n",
    "- $ \\text{DF}(t) $: Number of documents containing the term $ t $.\n",
    "- Adding $ +1 $ in numerator and denominator prevents division by zero.\n",
    "\n",
    "---\n",
    "\n",
    "### TF-IDF\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "\n",
    "üöß **Question** üöß\n",
    "\n",
    "Write a function that computes the TF-IDF matrix from a word count matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_to_tfidf(word_count_matrix):\n",
    "    # <-- TODO \n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: identifying similar documents\n",
    "\n",
    "- We can use TF-IDF to identify similar documents.\n",
    "- To quickly compare two documents, we can compute the cosine similarity between their TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vec = vectorizer.fit_transform(X.apply(preprocess_text))\n",
    "\n",
    "query = \"i like film with boats\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöß **Question** üöß\n",
    "\n",
    "Find the most similar text in $X$ ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limitations of frequency-based vectorization\n",
    "\n",
    "- Each word is treated independently, regardless of its context.\n",
    "- The order of words is not taken into account.\n",
    "- Semantic information is not captured.\n",
    "- And other ideas of limitations ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word embeddings\n",
    "\n",
    "**A Intuitive example**: Suppose we are interested in classifying texts in 4 categories: \"World\", \"Sports\", \"Business\", \"Sci/Tech\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word embeddings\n",
    "- We could manually design a vector for each word.\n",
    "- For instance, over the dimensions [World, Sported, Business, Sci/Tech]:\n",
    "  - \"Football\" could be [0, 1, 0, 0]\n",
    "  - \"Microsoft\" could be [0, 0, 0.5, 0.5]\n",
    "  - \"Olympics\" could be [0.25, 0.75, 0, 0]\n",
    "- But doing this naively would be very inefficient.\n",
    "  - Hard to design the vectors.\n",
    "  - Hard to compute them\n",
    "  - Does not account for the context of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning word embeddings\n",
    "\n",
    "- Instead, we can learn those embeddings from the data!\n",
    "- Same idea than with neural networks: we learn the weights of the embeddings from the data.\n",
    "\n",
    "**In practice**\n",
    "- Set a dimension $d$ for the embeddings (e.g. 100).\n",
    "- For each word, initialize a random vector of size $d$.\n",
    "- Train a model to predict something from the embeddings.\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "üöß **Question** üöß\n",
    "\n",
    "Add the necessary code in the following class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class WordEmbeddingClassifier(BaseTextClassifier):\n",
    "    def __init__(self, n_classes, voc_size, d_embed, tokenizer, lr=1e-3, n_epochs=10):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.voc_size = voc_size\n",
    "        self.lr = lr\n",
    "        self.d_embed = d_embed\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "    def predict(self, text: str) -> int:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = WordEmbeddingClassifier(\n",
    "    voc_size=tokenizer.num_words,\n",
    "    n_classes=2,\n",
    "    d_embed=2,\n",
    "    tokenizer=tokenizer,\n",
    "    lr=0.01,\n",
    "    n_epochs=10,\n",
    ")\n",
    "\n",
    "classifier.fit(X_train.tolist(), y_train)\n",
    "\n",
    "print(classifier.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the embedding matrix of a review\n",
    "\n",
    "text = \"a very bad movie i hated it.\"\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens = torch.tensor([t for t in tokens if t != -1])  # Ignore invalid tokens\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedded = classifier.embedding(tokens)  # Shape: (seq_len, d_embed)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(embedded.T, cmap=\"viridis\", aspect=\"auto\")\n",
    "plt.xticks(\n",
    "    range(len(tokens)), [tokenizer.id_to_token[t.item()] for t in tokens], rotation=45\n",
    ")\n",
    "plt.ylabel(\"Embedding dimension\")\n",
    "\n",
    "plt.colorbar(label=\"Embedding value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word embeddings in practice\n",
    "\n",
    "- Word embeddings can be very powerful but costly to learn.\n",
    "- An innovative idea was to pre-train word embeddings on a large corpus.\n",
    "- This is the idea from the paper \"Distributed Representations of Words and Phrases and their Compositionality\" by Mikolov et al. (2013), which introduced the Word2Vec model.\n",
    "- \n",
    "### Word2Vec\n",
    "- Word2Vec is a contrastive learning model.\n",
    "- Key Idea: Words with similar contexts (neighboring words) have similar vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Word2Vec: Key Ideas with Math\n",
    "\n",
    "1. **Select a Word and Context**  \n",
    "   - Randomly sample a word $w$ from a document.  \n",
    "   - Define its **positive context** as the words within a window of size $R$ around $w$, excluding $w$ itself:  \n",
    "     $$\n",
    "     \\mathcal{C}_{\\text{pos}} = [w_{i-R}, \\dots, w_{i-1}, w_{i+1}, \\dots, w_{i+R}].\n",
    "     $$\n",
    "\n",
    "2. **Negative Examples**  \n",
    "   - To balance the training, introduce **negative examples** by sampling $2KR$ random words from the vocabulary $\\mathcal{V}$:  \n",
    "     $$\n",
    "     \\mathcal{C}_{\\text{neg}} = \\{c_1, \\dots, c_{2KR}\\}, \\quad c_j \\in \\mathcal{V}.\n",
    "     $$\n",
    "\n",
    "3. **Embedding Words**  \n",
    "   - Represent the target word $w$ as a vector $\\mathbf{v}_w \\in \\mathbb{R}^d$ using an embeddings table $\\mathcal{E}_w$.  \n",
    "   - Similarly, map each word in $\\mathcal{C}_{\\text{pos}}$ and $\\mathcal{C}_{\\text{neg}}$ to vectors:  \n",
    "     $$\n",
    "     \\mathbf{v}_{\\mathcal{C}_{\\text{pos}}} \\in \\mathbb{R}^{2R \\times d}, \\quad \\mathbf{v}_{\\mathcal{C}_{\\text{neg}}} \\in \\mathbb{R}^{2KR \\times d}.\n",
    "     $$\n",
    "\n",
    "4. **Compute Similarity**  \n",
    "   - For each context word $c$ in $\\mathcal{C}_{\\text{pos}} \\cup \\mathcal{C}_{\\text{neg}}$, compute the similarity with $w$ using the dot product:  \n",
    "     $$\n",
    "     s = \\mathbf{v}_c \\cdot \\mathbf{v}_w.\n",
    "     $$  \n",
    "   - Train the model to maximize the similarity between $w$ and the positive context words, and minimize the similarity with the negative context words.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
